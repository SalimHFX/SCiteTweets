{"cells":[{"cell_type":"markdown","metadata":{"id":"12zXxnbaj-iv"},"source":["# Initialize"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32523,"status":"ok","timestamp":1713187172449,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"},"user_tz":-120},"id":"4c4mq1ilkCL0","outputId":"b2b09724-52a3-4cdd-fc0d-9106269273a3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n","/content/drive/MyDrive/Stage_Wassim_2024/NoteBooks/DataSet\n","annotations.tsv        model_SBU.pth       train.jsonl     twt_train.jsonl\n","Longformer_On_TWT.pth  SciBert_On_TWT.pth  twt_FULL.jsonl  val.jsonl\n","model_LFS.pth          test.jsonl          twt_test.jsonl\n"]}],"source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")\n","#%cd Use This cell to access your data folder\n","%ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dosnTEG0lXNi"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from tqdm import tqdm\n","import json\n","import random\n","from typing import AnyStr, List, Tuple, Callable, Union\n","import torch\n","\n","from functools import partial\n","from pathlib import Path\n","import gc\n","\n","from torch import nn\n","from torch.utils.data import Dataset, Subset, DataLoader\n","from transformers import get_linear_schedule_with_warmup, PreTrainedTokenizer, AdamW, AutoModelForSequenceClassification, AutoTokenizer, AutoConfig, AutoModel\n","from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n","from sklearn.model_selection import StratifiedKFold, KFold"]},{"cell_type":"markdown","metadata":{"id":"Ehb_wKBKrU1r"},"source":["# Classes / Functions\n"]},{"cell_type":"markdown","metadata":{"id":"9bN2pP6POZ3N"},"source":["## Data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9hQTFx04Odwa"},"outputs":[],"source":["LABELS = {\n","    'non-check-worthy': 0,\n","    'check-worthy': 1\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuB1ys9qQkIX"},"outputs":[],"source":["def read_citation_detection_jsonl_single_line(jsonl_file: AnyStr):\n","\n","    with open(jsonl_file) as f:\n","        data = [json.loads(l.strip()) for l in f]\n","\n","    # Get sentences and labels\n","    dataset = [[s['text'], LABELS[s['label']]] for d in tqdm(data, desc=\"Processing data\") for s in d['samples']]\n","\n","    return pd.DataFrame(dataset, columns=['text', 'label'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U2C_j8W7BAku"},"outputs":[],"source":["def read_citation_detection_jsonl(jsonl_file: AnyStr):\n","\n","    with open(jsonl_file) as f:\n","        dataset = [json.loads(l.strip()) for l in f]\n","\n","    processed_dataset = []\n","    for item in tqdm(dataset):\n","        texts = [s['text'] for s in item['samples']]\n","        labels = [LABELS[s['label']] for s in item['samples']]\n","        processed_dataset.append({\n","            'texts': texts,\n","            'labels': labels\n","        })\n","\n","    return processed_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZuMSNTI2psEc"},"outputs":[],"source":["def collate_batch_transformer(pad_token_id: int, input_data: Tuple) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","\n","  input_ids = [i[0][0] for i in input_data]\n","  masks = [i[1][0] for i in input_data]\n","  labels = [i[2] for i in input_data]\n","\n","  max_length = max([len(i) for i in input_ids])\n","\n","  input_ids =[(i + [pad_token_id] * (max_length-len(i))) for i in input_ids]\n","  masks = [(m+[0]*(max_length-len(m))) for m in masks]\n","\n","  assert(all(len(i) == max_length for i in input_ids))\n","  assert(all(len(m) == max_length for m in masks))\n","\n","  return torch.tensor(input_ids),torch.tensor(masks),torch.tensor(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"76ctflHdYav1"},"outputs":[],"source":["def collate_batch_transformer_with_weight(pad_token_id: int, input_data: Tuple) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n","    return collate_batch_transformer(pad_token_id, input_data) + (torch.tensor([i[3] for i in input_data]),)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SLKDuNeECcLB"},"outputs":[],"source":["def text_to_batch_transformer(text: List, tokenizer: PreTrainedTokenizer) -> Tuple[List, List]:\n","\n","    max_length = min(512, tokenizer.model_max_length)\n","    input_ids = [tokenizer.encode(t, add_special_tokens=True, max_length=max_length, truncation=True, verbose=False) for t in text]\n","\n","    masks = [[1] * len(i) for i in input_ids]\n","\n","    return input_ids, masks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DBHGG7L2B9Pe"},"outputs":[],"source":["def collate_sequence_batch_transformer(pad_token_id: int, input_data: Tuple) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n","\n","    input_ids = [i[0][0] for i in input_data]\n","    masks = [i[1][0] for i in input_data]\n","    labels = [i[2] for i in input_data]\n","    seq_lens = [len(i[2]) for i in input_data]\n","\n","    max_length = max([len(i) for i in input_ids])\n","\n","    input_ids = [(i + [pad_token_id] * (max_length - len(i))) for i in input_ids]\n","    masks = [(m + [0] * (max_length - len(m))) for m in masks]\n","\n","    max_seq_len = max(seq_lens)\n","    labels = [(l + [-1] * (max_seq_len - len(l))) for l in labels]\n","    seq_masks = [[1] * s + [0] * (max_seq_len - s) for s in seq_lens]\n","\n","    assert (all(len(i) == max_length for i in input_ids))\n","    assert (all(len(m) == max_length for m in masks))\n","    assert (all(len(l) == max_seq_len for l in labels))\n","\n","    return torch.tensor(input_ids), torch.tensor(masks), torch.tensor(labels), torch.tensor(seq_masks)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"du1BSbgvC4E-"},"outputs":[],"source":["def text_to_sequence_batch_transformer(text: List, tokenizer: PreTrainedTokenizer) -> Tuple[List, List]:\n","\n","    max_length = min(512, tokenizer.model_max_length)\n","    input_ids = [tokenizer.encode(t, add_special_tokens=True, max_length=max_length, truncation=True, verbose=False) for t in text]\n","    input_ids = [[id_ for i,sent in enumerate(input_ids) for j,id_ in enumerate(sent) if (i == 0 or j != 0)][:tokenizer.model_max_length]]\n","    input_ids[0][-1] = tokenizer.sep_token_id\n","\n","\n","    masks = [[1] * len(i) for i in input_ids]\n","\n","    return input_ids, masks\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HCnxUZdnCNkP"},"outputs":[],"source":["class TransformerSingleSentenceDataset(Dataset):\n","\n","    def __init__(self, jsonl_file: AnyStr, tokenizer, tokenizer_fn: Callable = text_to_batch_transformer):\n","\n","        self.dataset = read_citation_detection_jsonl_single_line(jsonl_file)\n","        self.tokenizer = tokenizer\n","        self.tokenizer_fn = tokenizer_fn\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx: int):\n","        row = self.dataset.iloc[idx].values\n","        # Calls the text_to_batch function\n","        input_ids, masks = self.tokenizer_fn([row[0]], self.tokenizer)\n","        label = row[1]\n","        return input_ids, masks, label\n","\n","    def getLabels(self):\n","        return self.dataset.values[:,1]"]},{"cell_type":"code","source":["class TransformerSingleSentenceDataset_forFolds(Dataset):\n","\n","    def __init__(self, jsonl_file: AnyStr, tokenizer, tokenizer_fn: Callable = text_to_batch_transformer):\n","\n","        self.dataset = read_citation_detection_jsonl_single_line(jsonl_file)\n","        self.tokenizer = tokenizer\n","        self.tokenizer_fn = tokenizer_fn\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx: int):\n","        row = self.dataset.iloc[idx].values\n","        return row[0], row[1]\n","\n","    def getLabels(self):\n","        return self.dataset.values[:,1]"],"metadata":{"id":"IxU91YiEYLaA"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GVwZ44BL-UAE"},"outputs":[],"source":["class TransformerMultiSentenceDataset(Dataset):\n","\n","    def __init__(self, jsonl_file: AnyStr, tokenizer, tokenizer_fn: Callable = text_to_sequence_batch_transformer):\n","\n","        self.dataset = read_citation_detection_jsonl(jsonl_file)\n","        self.tokenizer = tokenizer\n","        self.tokenizer_fn = tokenizer_fn\n","\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx: int):\n","        row = self.dataset[idx]\n","        sents = row['texts']\n","        labels = row['labels']\n","        # Calls the text_to_batch function\n","        input_ids, masks = self.tokenizer_fn(sents, self.tokenizer)\n","        n_labels = sum(np.array(input_ids)[0] == self.tokenizer.sep_token_id)\n","        return input_ids, masks, labels[:n_labels]\n","\n","    def getLabels(self, indices=None):\n","        if indices is None:\n","           all_labels=[row['labels'] for row in self.dataset]\n","           return np.concatenate(all_labels)\n","        else:\n","            selected_labels = [self.dataset[i]['labels'] for i in indices]\n","            return np.concatenate(selected_labels)  # Concatenate selected labels into one array"]},{"cell_type":"code","source":["def to_Xy(dataset):\n","  X = []\n","  y = []\n","\n","  # Iterate through the dataset\n","  for idx in range(len(dataset)):\n","      input_ids, masks, label = dataset[idx]\n","      X.append((input_ids, masks))\n","      y.append(label)\n","\n","  return X,y"],"metadata":{"id":"HduMVbl1Psdq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nuPxsPMH8bYc"},"source":["## Metric\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HAhg1D5i8hqR"},"outputs":[],"source":["# Classic metrics\n","\n","def accuracy(preds: np.ndarray, labels: np.ndarray) -> float:\n","    return np.sum(preds == labels).astype(np.float32) / float(labels.shape[0])\n","\n","def acc_f1(preds: List, labels: List, averaging: AnyStr = 'binary') -> Tuple[float, float, float, float]:\n","    acc = accuracy(preds, labels)\n","    P, R, F1, _ = precision_recall_fscore_support(labels, preds, average=averaging)\n","\n","    return acc,P,R,F1\n","\n","def average_precision(labels: np.ndarray, order: np.ndarray) -> float:\n","    \"\"\"\n","    Calculates the average precision of a ranked list\n","    :param labels: True labels of the items\n","    :param order: The ranking order\n","    :return: Average precision\n","    \"\"\"\n","    j = 0\n","    ap = 0\n","    for i, v in enumerate(labels[order]):\n","        if v == 1:\n","            j += 1\n","            ap += j / (i + 1)\n","    return ap / j"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wgugAwZr87wJ"},"outputs":[],"source":["class ClassificationEvaluator:\n","\n","    def __init__(\n","            self,\n","            dataset: Dataset,\n","            device: torch.device,\n","            num_labels: int = 2,\n","            averaging: AnyStr = 'binary',\n","            pad_token_id: int = None,\n","            mlm: bool = False,\n","            sequence_modeling: bool = False\n","    ):\n","\n","        self.dataset = dataset\n","        if isinstance(dataset, Subset):\n","            self.all_labels = list(dataset.dataset.getLabels(dataset.indices))\n","        else:\n","            self.all_labels = np.array([element[2] for element in dataset])\n","\n","        if sequence_modeling:\n","            collator = collate_sequence_batch_transformer\n","        else:\n","            collator = collate_batch_transformer\n","\n","\n","        if pad_token_id is None:\n","            collate_fn = partial(collator, dataset.tokenizer.pad_token_id)\n","        else:\n","            collate_fn = partial(collator, pad_token_id)\n","\n","        self.dataloader = DataLoader(\n","            dataset,\n","            batch_size=32,\n","            collate_fn=collate_fn\n","        )\n","        self.device = device\n","        self.averaging = averaging\n","        self.num_labels = num_labels\n","        self.mlm = mlm\n","        self.pad_token_id = pad_token_id\n","        self.sequence_modeling = sequence_modeling\n","\n","    def predict(\n","            self,\n","            model: torch.nn.Module\n","    ) -> Tuple:\n","        model.eval()\n","        with torch.no_grad():\n","            labels_all = []\n","            logits_all = []\n","            losses_all = []\n","            preds_all = []\n","            for batch in tqdm(self.dataloader, desc=\"Evaluation\"):\n","                if isinstance(batch, dict):\n","                    batch = [batch['input_ids'], batch['labels']]\n","                batch = tuple(t.to(self.device) if isinstance(t, torch.Tensor) else t for t in batch)\n","                input_ids = batch[0]\n","                if self.mlm:\n","                    labels = batch[1]\n","                    masks = torch.tensor(input_ids != self.pad_token_id).to(self.device)\n","                else:\n","                    masks = batch[1]\n","                    labels = batch[2]\n","\n","                if self.sequence_modeling:\n","                    # For CRF\n","                    seq_mask = batch[3]\n","                    outputs = model(input_ids, attention_mask=masks, seq_mask=seq_mask, labels=labels)\n","                    labels_all.extend([l for seq in list(labels.detach().cpu().numpy()) for l in seq if l != -1])\n","                else:\n","                    input_dict = {'input_ids': input_ids, 'attention_mask': masks}\n","                    input_dict['labels'] = labels\n","                    outputs = model(**input_dict)\n","                    (loss,logits) = (outputs['loss'],outputs['logits'])\n","\n","\n","                    labels_all.extend(list(labels.detach().cpu().numpy()))\n","                if not self.mlm:\n","                    # For memory issues\n","                    logits_all.extend(list(outputs[1].detach().cpu().numpy()))\n","\n","                losses_all.append(outputs[0].item())\n","\n","                if self.sequence_modeling:\n","                    masks = (labels != -1)\n","                    preds = model.decode(outputs[1], masks)\n","                    preds_all.extend([t for seq in preds for t in seq])\n","                elif not self.mlm:\n","                    preds = np.argmax(outputs[1].detach().cpu().numpy().reshape(-1, self.num_labels), axis=-1)\n","                    preds_all.extend([p for p in preds])\n","        if not self.mlm:\n","            assert len(labels_all) == len(self.all_labels)\n","            assert len(logits_all) == len(self.all_labels)\n","            assert len(preds_all) == len(self.all_labels)\n","        return labels_all, logits_all, losses_all, preds_all\n","\n","    def roc_auc(self, model: torch.nn.Module):\n","        labels_all, logits_all, losses_all = self.predict(model)\n","        logits = np.asarray(logits_all).reshape(-1, self.num_labels)\n","        labels = np.asarray(labels_all).reshape(-1)\n","        fpr, tpr, _ = roc_curve(labels, logits[:, 1])\n","        roc_auc = auc(fpr, tpr)\n","\n","        return fpr, tpr, roc_auc\n","\n","    def evaluate(\n","            self,\n","            model: torch.nn.Module,\n","            plot_callbacks: List[Callable] = [],\n","            return_labels_logits: bool = False\n","    ) -> Tuple:\n","        labels_all, logits_all, losses_all, preds_all = self.predict(model)\n","        loss = sum(losses_all) / len(losses_all)\n","        if self.mlm:\n","            ret_vals = loss\n","        else:\n","            if self.averaging == 'binary' and self.num_labels > 2:\n","                preds_all = [p if p == 0 else 1 for p in preds_all]\n","            acc,P,R,F1 = acc_f1(np.asarray(preds_all), np.asarray(labels_all).reshape(-1), averaging=self.averaging)\n","            ret_vals = (loss, acc, P, R, F1)\n","\n","            # Plotting\n","            plots = []\n","            for f in plot_callbacks:\n","                plots.append(f(labels_all, logits_all))\n","\n","            if len(plots) > 0:\n","                ret_vals = (loss, acc, P, R, F1), plots\n","\n","        # Labels and logits\n","        if return_labels_logits:\n","            ret_vals = ret_vals + (labels_all, logits_all, preds_all,)\n","\n","        return ret_vals"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GqJIosbWD0rl"},"outputs":[],"source":["class AutoTransformerForSentenceSequenceModeling(nn.Module):\n","    \"\"\"\n","       Implements a transformer which performs sequence classification on a sequence of sentences\n","    \"\"\"\n","\n","    def __init__(self, transformer_model: AnyStr, num_labels: int = 2, sep_token_id: int = 2):\n","        super(AutoTransformerForSentenceSequenceModeling, self).__init__()\n","\n","        config = AutoConfig.from_pretrained(transformer_model)\n","        self.config = config\n","        self.xformer = AutoModel.from_pretrained(transformer_model, config=config)\n","\n","        # Pooling layers\n","        self.pooling = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.act = nn.Tanh()\n","\n","        # Create the classifier heads\n","        self.classifier = nn.Linear(config.hidden_size, num_labels)\n","        self.num_labels = num_labels\n","\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.sep_token_id = sep_token_id\n","\n","        #Loss function\n","        self.loss_fn = nn.CrossEntropyLoss()\n","\n","    def forward(\n","            self,\n","            input_ids=None,\n","            attention_mask=None,\n","            labels=None,\n","            lam=1.0\n","    ):\n","        outputs = self.xformer(\n","            input_ids,\n","            attention_mask=attention_mask,\n","\n","        )\n","\n","        # Gather all of the SEP hidden states\n","        hidden_states = outputs['last_hidden_state'].reshape(-1, self.config.hidden_size)\n","        locs = (input_ids == self.sep_token_id).view(-1)\n","        #(n * seq_len x d) -> (n * sep_len x d)\n","        sequence_output = hidden_states[locs]\n","        assert sequence_output.shape[0] == sum(locs)\n","        assert sequence_output.shape[1] == self.config.hidden_size\n","\n","        pooled_output = self.pooling(sequence_output)\n","        pooled_output = self.dropout(self.act(pooled_output))\n","\n","        logits = self.classifier(pooled_output)\n","\n","        outputs = (logits,)\n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = lam * loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return {'loss': loss, 'logits': logits}"]},{"cell_type":"markdown","metadata":{"id":"MhTiYrbQ5SYv"},"source":["## Training\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8pEvQxh63Qph"},"outputs":[],"source":["class AbstractTransformerTrainer:\n","    \"\"\"\n","    An abstract class which other trainers should implement\n","    \"\"\"\n","    def __init__(\n","            self,\n","            model = None,\n","            device = None,\n","            tokenizer = None\n","    ):\n","\n","        self.model = model\n","        self.device = device\n","        self.tokenizer = tokenizer\n","\n","    def create_optimizer(self, lr: float, weight_decay: float=0.0):\n","        \"\"\"\n","        Create a weighted adam optimizer with the given learning rate\n","        \"\"\"\n","\n","        no_decay = ['bias', 'LayerNorm.weight']\n","        optimizer_grouped_parameters = [\n","            {'params': [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","             'weight_decay': weight_decay},\n","            {'params': [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","        ]\n","        return AdamW(optimizer_grouped_parameters, lr=lr)\n","\n","    def save(self, model_file: AnyStr):\n","        \"\"\"\n","        Saves the current model\n","        :return:\n","        \"\"\"\n","        if not Path(model_file).parent.exists():\n","            Path(model_file).parent.mkdir(parents=True, exist_ok=True)\n","\n","        save_model = self.model\n","\n","        torch.save(save_model.state_dict(), model_file)\n","\n","    def load(self, model_file: AnyStr, new_classifier: bool = False):\n","        \"\"\"\n","        Loads the model given by model_file\n","        \"\"\"\n","        model_dict = self.model.state_dict()\n","        load_model = self.model\n","        if new_classifier:\n","            weights = {k: v for k, v in torch.load(model_file, map_location=lambda storage, loc: storage).items() if \"classifier\" not in k and \"pooler\" not in k}\n","            model_dict.update(weights)\n","            load_model.load_state_dict(model_dict)\n","        else:\n","            weights = torch.load(model_file, map_location=lambda storage, loc: storage)\n","            model_dict.update(weights)\n","            load_model.load_state_dict(model_dict)\n","\n","    def freeze(self, exclude_params: List=[]):\n","        \"\"\"\n","        Freeze the model weights\n","        :return:\n","        \"\"\"\n","        for n,p in self.model.named_parameters():\n","            if n not in exclude_params:\n","                p.requires_grad = False\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Utn5i4pC5sEi"},"outputs":[],"source":["class TransformerClassificationTrainer(AbstractTransformerTrainer):\n","    \"\"\"\n","    A class to encapsulate all of the training and evaluation of a\n","    transformer model for classification\n","    \"\"\"\n","    def __init__(\n","            self,\n","            transformer_model: Union[AnyStr, torch.nn.Module],\n","            device: torch.device,\n","            num_labels: Union[int, List],\n","            tokenizer=None\n","    ):\n","        if type(num_labels) != list:\n","            num_labels = [num_labels]\n","\n","        if type(transformer_model) == str:\n","            self.model_name = transformer_model\n","            # Create the model\n","            config = AutoConfig.from_pretrained(transformer_model, num_labels=num_labels[0])\n","            self.model = AutoModelForSequenceClassification.from_pretrained(transformer_model, config=config)\n","            self.model.cuda()\n","            self.tokenizer = AutoTokenizer.from_pretrained(transformer_model)\n","        else:\n","            self.model_name = 'custom'\n","            self.model = transformer_model\n","            self.tokenizer = tokenizer\n","            if tokenizer == None:\n","                print(\"WARNING: No tokenizer passed to trainer, incorrect padding token may be used.\")\n","\n","        self.device = device\n","        self.num_labels = num_labels\n","\n","    def evaluate(\n","            self,\n","            validation_dset: Dataset,\n","            eval_averaging: AnyStr = 'binary',\n","            return_labels_logits: bool = False,\n","            sequence_modeling : bool = False,\n","    ):\n","        \"\"\"\n","        Runs a round of evaluation on the given dataset\n","        \"\"\"\n","        if self.tokenizer is not None:\n","            pad_token_id = self.tokenizer.pad_token_id\n","        else:\n","            pad_token_id = 0\n","        # Create the validation evaluator\n","        validation_evaluator = ClassificationEvaluator(\n","            validation_dset,\n","            self.device,\n","            num_labels=self.num_labels[0],\n","            averaging=eval_averaging,\n","            pad_token_id=pad_token_id,\n","            sequence_modeling=sequence_modeling,\n","        )\n","        return validation_evaluator.evaluate(self.model, return_labels_logits=return_labels_logits)\n","\n","    def train(\n","            self,\n","            train_dset: Union[List, Dataset],\n","            validation_dset: Dataset,\n","            logger = None,\n","            lr: float = 3e-5,\n","            n_epochs: int = 2,\n","            batch_size: int = 8,\n","            weight_decay: float = 0.0,\n","            warmup_steps: int = 200,\n","            log_interval: int = 1,\n","            metric_name: AnyStr = 'accuracy',\n","            patience: int = 10,\n","            model_file: AnyStr = \"model.pth\",\n","            class_weights=None,\n","            use_scheduler: bool = True,\n","            eval_averaging: AnyStr = 'binary',\n","            lams: List = None,\n","            clip_grad: float = None,\n","            num_dataset_workers: int = 10\n","    ):\n","\n","        train_dset = [train_dset]\n","        if self.tokenizer is not None:\n","            pad_token_id = self.tokenizer.pad_token_id\n","        else:\n","            pad_token_id = 0\n","        if lams is None:\n","            lams = [1.0] * len(train_dset)\n","\n","        if (isinstance(class_weights, str) and class_weights == 'CRF'):\n","            collate_fn = partial(collate_sequence_batch_transformer, pad_token_id)\n","        else:\n","            collate_fn = partial(collate_batch_transformer, pad_token_id)\n","\n","        # Create the loss function\n","        if class_weights is None:\n","            loss_fn = torch.nn.CrossEntropyLoss()\n","\n","        elif (isinstance(class_weights, str) and class_weights == 'sample_based_weight'):\n","            loss_fn = torch.nn.CrossEntropyLoss(reduction='none')\n","            collate_fn = partial(collate_batch_transformer_with_weight, pad_token_id)\n","\n","        elif isinstance(class_weights, str) and class_weights == 'balanced':\n","            if isinstance(train_dset[0], Subset):\n","                labels = train_dset[0].dataset.getLabels().astype(np.int64)\n","            else:\n","                labels = np.array([element[2] for element in train_dset[0]]).astype(np.int64)\n","            weight = torch.tensor(len(labels) / (self.num_labels[0] * np.bincount(labels)))\n","            weight = weight.type(torch.FloatTensor).to(self.device)\n","            loss_fn = torch.nn.CrossEntropyLoss(weight=weight)\n","\n","        elif isinstance(class_weights, str) and class_weights == 'CRF':\n","            loss_fn = self.model.loss_fn\n","        else:\n","            loss_fn = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights).type(torch.FloatTensor).to(self.device))\n","\n","        # Create the training dataloader(s)\n","        train_dls = [DataLoader(\n","            ds,\n","            batch_size=batch_size,\n","            shuffle=True,\n","            collate_fn=collate_fn,\n","            num_workers=num_dataset_workers\n","        ) for ds in train_dset]\n","\n","        # Create the optimizer\n","        optimizer = self.create_optimizer(lr, weight_decay)\n","\n","        total = sum(len(dl) for dl in train_dls)\n","\n","        if use_scheduler:\n","            # Create the scheduler\n","            scheduler = get_linear_schedule_with_warmup(\n","                optimizer,\n","                warmup_steps,\n","                n_epochs * len(train_dls[0])#total\n","            )\n","\n","        # Set up metric tracking\n","        best_metric = 0.0\n","        patience_counter = 0\n","        # Main training loop\n","        for ep in range(n_epochs):\n","            # Training loop\n","            dl_iters = [iter(dl) for dl in train_dls]\n","            dl_idx = list(range(len(dl_iters)))\n","\n","            finished = [0] * len(dl_iters)\n","            i = 0\n","            with tqdm(total=total, desc=\"Training\") as pbar:\n","                while sum(finished) < len(dl_iters):\n","                    random.shuffle(dl_idx)\n","                    for d in dl_idx:\n","                        task_dl = dl_iters[d]\n","                        try:\n","                            batch = next(task_dl)\n","                        except StopIteration:\n","                            finished[d] = 1\n","                            continue\n","\n","                        self.model.train()\n","                        optimizer.zero_grad()\n","\n","                        batch = tuple(t.to(self.device) for t in batch)\n","                        input_ids = batch[0]\n","                        masks = batch[1]\n","                        labels = batch[2]\n","                        original_labels = labels\n","                        outputs = self.model(input_ids, attention_mask=masks)\n","                        logits = outputs['logits'].to(self.device)\n","\n","                        # Calculate what the weight of the loss should be\n","                        loss_weight = lams[d]\n","                        if (isinstance(class_weights, str) and class_weights == 'sample_based_weight'):\n","                            sample_weight = batch[-1].to(self.device)\n","                            loss_weight *= sample_weight\n","                            loss = (loss_weight * loss_fn(logits.view(-1, self.num_labels[d]), labels.view(-1))).mean()\n","                        elif (isinstance(class_weights, str) and class_weights == 'CRF'):\n","                            seq_mask = batch[3].to(self.device)\n","                            loss = loss_fn(logits, labels, seq_mask)\n","                        else:\n","                            loss = loss_weight * loss_fn(logits.view(-1, self.num_labels[d]), labels.view(-1))\n","\n","\n","                        if i % log_interval == 0 and logger is not None:\n","                            logger.log({\"Loss\": loss.item()})\n","                        loss.backward()\n","                        # Clip gradients\n","                        if clip_grad:\n","                          torch.nn.utils.clip_grad_norm_(self.model.parameters(), clip_grad)\n","                        optimizer.step()\n","                        i += 1\n","                        pbar.update(1)\n","                    if use_scheduler:\n","                        scheduler.step()\n","\n","            gc.collect()\n","\n","            # Inline evaluation\n","            (val_loss, acc, P, R, F1) = self.evaluate(validation_dset, eval_averaging, sequence_modeling=(class_weights == 'CRF'))\n","            if metric_name == 'accuracy':\n","                metric = acc\n","            else:\n","                metric = F1\n","                if eval_averaging is None:\n","                    # Macro average if averaging is None\n","                    metric = sum(F1) / len(F1)\n","\n","            print(f\"{metric_name}: {metric}\")\n","\n","            if logger is not None:\n","                # Log\n","                logger.log({\n","                    'Validation accuracy': acc,\n","                    'Validation Precision': P,\n","                    'Validation Recall': R,\n","                    'Validation F1': F1,\n","                    'Validation loss': val_loss}\n","                )\n","            else:\n","                print({\n","                    'Validation accuracy': acc,\n","                    'Validation Precision': P,\n","                    'Validation Recall': R,\n","                    'Validation F1': F1,\n","                    'Validation loss': val_loss}\n","                )\n","\n","            # Saving the best model and early stopping\n","            # if val_loss < best_loss:\n","            #self.save(f\"model_epoch_{ep}.pth\")\n","            if metric > best_metric:\n","                best_metric = metric\n","                if logger != None:\n","                    logger.log({f\"best_{metric_name}\": metric})\n","                else:\n","                    print(f\"Best {metric_name}: {metric}\")\n","                self.save(model_file)\n","                patience_counter = 0\n","            else:\n","                patience_counter += 1\n","                # Stop training once we have lost patience\n","                if patience_counter == patience:\n","                    break\n","\n","            gc.collect()\n","\n","        self.load(model_file)\n"]},{"cell_type":"markdown","metadata":{"id":"BLcFXvsoK9_k"},"source":["## SetUp"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iJBgfX5KnS3R"},"outputs":[],"source":["def enforce_reproducibility(seed=1000):\n","    # Sets seed manually for both CPU and CUDA\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    # For atomic operations there is currently\n","    # no simple way to enforce determinism, as\n","    # the order of parallel operations is not known.\n","    # CUDNN\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    # System based\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    print(\"seed set to : \",seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2cjGlYFBJnem"},"outputs":[],"source":["def check_attributes (trainer, config, datareader) :\n","  print(\"Cheking model on gpu : \")\n","  print(\"-----------------------\")\n","  count = 0\n","  for i in next(trainer.model.parameters()) :\n","    if i.is_cuda :\n","      count += 1\n","  if count == len(next(trainer.model.parameters())) :\n","    print(\"everything is ready and on the GPU\")\n","\n","  print('\\n')\n","\n","  print(\"Checking important model attributes : \")\n","  print(\"--------------------------------------\")\n","  print('The model  : \\n')\n","  print(trainer.model)\n","  print('\\n')\n","  print('Tokenizer : ')\n","  print(trainer.tokenizer)\n","  print('\\n')\n","  print('Pad token id : ')\n","  print(trainer.tokenizer.pad_token_id)\n","  print('\\n')\n","  print('DataReaderClass : ')\n","  print(datareader)\n","  print('\\n')\n","  print(\"Config:\")\n","  print(\"\\n\")\n","  for key, value in config.items():\n","      print(f\"{key}: {value}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2DMNmrd2KbCB"},"outputs":[],"source":["def load_device():\n","  if torch.cuda.is_available() :\n","    print('Training on GPU')\n","    device = torch.device(\"cuda:0\")\n","    print('======> Success !')\n","    print('Name of GPU : ',torch.cuda.get_device_name(0))\n","    print('device is : ',device)\n","  else :\n","    print('Training on CPU')\n","    device = torch.device(\"cpu\")\n","    print('======> Success !')\n","    print('device is : ',device)\n","  return device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cTZB5LyEKjIS"},"outputs":[],"source":["def device_info():\n","  print('Is this notebook using GPU : ',torch.cuda.is_available())\n","  if torch.cuda.is_available() :\n","    print('\\n')\n","    print('How many GPUs are available : ',torch.cuda.device_count())\n","    print('The current GPUs used : ',torch.cuda.current_device())\n","    print('The name of the GPU : ',torch.cuda.get_device_name(0))\n","    print('\\n')\n","    !nvidia-smi\n","  else :\n","    print('\\n')\n","    print(' ================> This notebook is running on a CPU')"]},{"cell_type":"markdown","metadata":{"id":"_1t62yRTsJXf"},"source":["# Checking GPU"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":438,"status":"ok","timestamp":1713187179235,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"},"user_tz":-120},"id":"GVOpz07MtAyt","outputId":"8aba74e3-2b5d-4992-c76d-01f4bac1e01b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Is this notebook using GPU :  True\n","\n","\n","How many GPUs are available :  1\n","The current GPUs used :  0\n","The name of the GPU :  Tesla V100-SXM2-16GB\n","\n","\n","Mon Apr 15 13:19:38 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla V100-SXM2-16GB           Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   32C    P0              24W / 300W |      2MiB / 16384MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}],"source":["device_info()"]},{"cell_type":"markdown","metadata":{"id":"Zkb1-F24mIq2"},"source":["# Experiments"]},{"cell_type":"markdown","source":["## SciBERT"],"metadata":{"id":"OO4wnJpr8oSF"}},{"cell_type":"markdown","metadata":{"id":"Tx-K17cxsTnm"},"source":["### Experiment 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":726,"status":"ok","timestamp":1713187216124,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"},"user_tz":-120},"id":"ZaSNy_4EneC9","outputId":"b33efd94-9e5b-4572-d8bc-962254df7544"},"outputs":[{"output_type":"stream","name":"stdout","text":["seed set to :  1000\n"]}],"source":["#define the run name\n","\n","run = 'SBU'\n","\n","\n","# Setting Parameters and random seed\n","if run == 'SBU' :\n","  seed = 1000\n","  lr = 0.000001351\n","  weight_decay = 0.1\n","  warmup_steps = 300\n","  dropout_prob = 0.1\n","  batch_size = 4\n","  n_epochs = 3\n","  class_weights = None\n","  model_name = 'allenai/scibert_scivocab_uncased'\n","  pu_learning = None\n","  num_labels = 2\n","  use_scheduler = True\n","  n_gpu = 1\n","  sequence_modeling = False\n","else :\n","  seed = 1000\n","  lr = 0.00001112\n","  weight_decay = 0.0\n","  warmup_steps = 300\n","  dropout_prob = 0.1\n","  batch_size = 4\n","  n_epochs = 3\n","  class_weights = 'balanced'\n","  model_name = 'allenai/longformer-base-4096'\n","  pu_learning = None\n","  num_labels = 2\n","  use_scheduler = True\n","  n_gpu = 1\n","  sequence_modeling = False\n","\n","\n","\n","assert batch_size % n_gpu == 0, \"Batch must be divisible by the number of GPUs used\"\n","\n","\n","config = {\n","            \"model_used\": model_name,\n","            \"epochs\": n_epochs,\n","            \"learning_rate\": lr,\n","            \"warmup\": warmup_steps,\n","            \"weight_decay\": weight_decay,\n","            \"batch_size\": batch_size,\n","            \"seed\": seed,\n","            \"use_scheduler\": use_scheduler,\n","            \"balance_class_weight\": class_weights,\n","            \"pu_learning\": pu_learning,\n","            \"sequence_modeling\": sequence_modeling\n","        }\n","\n","enforce_reproducibility(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GesYgO-vqHD2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713187201327,"user_tz":-120,"elapsed":379,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"d487eb3f-15e9-4ce5-99c0-b65d03f52f06"},"outputs":[{"output_type":"stream","name":"stdout","text":["Training on GPU\n","======> Success !\n","Name of GPU :  Tesla V100-SXM2-16GB\n","device is :  cuda:0\n"]}],"source":["device = load_device()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rre2HmMGmOAP","colab":{"base_uri":"https://localhost:8080/","height":289,"referenced_widgets":["40051d90a7724e7b9895ca75731f7ac9","2791c002b68844e1be028f73ae67935b","1c6e8c5fdd624c3989336f2a61754526","caaaa55b33af478e89372b633a0a4bf1","2a3f5a91e3d946ffa7fa730687df5b8c","44b2102d5bf346deb349d4dc4af176b9","a4b1019b91d14fa69065d561cb1f5da3","1c93a7a55b2747eb8babbdef985c6793","ef8a95c8928c44cc972cc88e3c3867aa","f5ea5887b4f6450c81129bc256fc4ac1","d854240f786e44eda00a795c4a1aa59a","c7b3a8fdc667424d972954763edd9f01","b5b444322c0440a3bee1e1cae88d6b7f","bfd25ea1a22c4fc4a4a8ad1cada12e01","440f80f4a4d64821a8480d08f8375df8","8a9b5d0b3ea041829e017d80f79c6173","0b94ffb7371d4479ab2de48d73d0c82b","1bf8d0a9bb414611a58f2212489d3559","cf610fe5cd8b46939535ff3457d1f2c1","d7321657a9d647fb8563626666e044e5","50a180f43c2f4721bac9266d8c52029d","78bda59e38ce406e8718fd0608f145b2"]},"executionInfo":{"status":"ok","timestamp":1713187243819,"user_tz":-120,"elapsed":24995,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"111bd1b0-b476-4e64-a240-a11b9eb04c67"},"outputs":[{"output_type":"stream","name":"stdout","text":["The model used will be :  allenai/scibert_scivocab_uncased\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40051d90a7724e7b9895ca75731f7ac9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7b3a8fdc667424d972954763edd9f01"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Processing data: 100%|██████████| 169015/169015 [00:01<00:00, 124767.22it/s]\n","Processing data: 100%|██████████| 20990/20990 [00:00<00:00, 58346.34it/s]\n","Processing data: 100%|██████████| 20995/20995 [00:00<00:00, 66017.89it/s]\n"]}],"source":["# Getting the data\n","\n","train_data_loc = \"train.jsonl\"\n","valid_data_loc = \"val.jsonl\"\n","test_data_loc = \"test.jsonl\"\n","\n","\n","# Initiating the model, Tokenizer and tokenizer function\n","if sequence_modeling :\n","  DataReaderClass = TransformerMultiSentenceDataset\n","  tokenizer = AutoTokenizer.from_pretrained(model_name)\n","  model = AutoTransformerForSentenceSequenceModeling(\n","            model_name,\n","            num_labels=num_labels,\n","            sep_token_id=tokenizer.sep_token_id\n","        ).to(device)\n","else :\n","  DataReaderClass = TransformerSingleSentenceDataset\n","  model = model_name\n","  print('The model used will be : ',model)\n","  print('\\n')\n","  tokenizer = AutoTokenizer.from_pretrained(model)\n","  tokenizer_fn = text_to_batch_transformer\n","\n","\n","\n","# Formatting the data and tokenizing it using the class and tokenizer function\n","train_dset = DataReaderClass(train_data_loc, tokenizer, tokenizer_fn=tokenizer_fn)\n","valid_dset = DataReaderClass(valid_data_loc, tokenizer, tokenizer_fn=tokenizer_fn)\n","test_dset = DataReaderClass(test_data_loc, tokenizer, tokenizer_fn=tokenizer_fn)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9nysC1HhP8-"},"outputs":[],"source":["# Creating the training class\n","trainer = TransformerClassificationTrainer(\n","        model,\n","        device,\n","        num_labels=num_labels,\n","        tokenizer=tokenizer,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M_Xs7w65EjjI"},"outputs":[],"source":["check_attributes(trainer, config, DataReaderClass)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pMIUYmGg2sfV"},"outputs":[],"source":["# Training the model\n","trainer.train(\n","        train_dset,\n","        valid_dset,\n","        weight_decay=weight_decay,\n","        model_file=\"model_SBU.pth\",\n","        class_weights=class_weights,\n","        metric_name='F1',\n","        logger=None,\n","        lr=lr,\n","        warmup_steps=warmup_steps,\n","        n_epochs=n_epochs,\n","        batch_size=batch_size,\n","        use_scheduler=use_scheduler\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FNiZ6hWbYFQS"},"outputs":[],"source":["(test_loss, acc, P, R, F1) = trainer.evaluate(test_dset)\n","\n","print('test-loss = ', test_loss)\n","print('test-acc = ', acc)\n","print('test-P = ', P)\n","print('test-R = ', R)\n","print('test-F1 = ', F1)\n"]},{"cell_type":"markdown","metadata":{"id":"KEj-IuIAesxC"},"source":["### Experiment 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kEvvhUKtNvkh","colab":{"base_uri":"https://localhost:8080/","height":428,"referenced_widgets":["93dc5767cd7243d8a3fb53509ab955b6","fb9ba356864944d3ad6b795964dd6aea","629e61b7ba0f49aca7d70999aa240cba","771de774558244919b5249d68fea4ade","dc90e1022e5d4f2b88e55c1e5543e3d6","91bbea735287448a85b6bc6f1f3c1190","3f46e2e2b6844c2e94c992129116b1d9","d1f25c7bf4f043a183f4e7524ffdf6f1","f9403afb44c241c4a5b2348ca119642f","450c2965248245ea94d33ff964772229","0f40a0d889a8440b8121f35de1a1cccc","93010b298a5d43ae80bb8f3290ab1e74","b9b10eb7390f4471bba2506c55d20308","28c238dc5b15412f900b82164bd7837b","c43c2d8cf1c74f4594896bc49352f512","a33c56c1ed834bfc889e4c9d728b1e84","95f994316d68460e87b3724318ecc81f","359bf94703b3406b942d9c48cc38de18","f791691cff054d638a56acf2d63e20b5","cf9e66c5b0fa4816a3c32a8b7e3dcd81","2b26a3199b754562a82cef6ca6c57d75","b66b30b94a974c07b5280b269a97c005","459e1d6c79484555a99cd8bc749a22a8","e68c79b15ea94bfe9376a204c87b0660","49488eb5165f4b9987a92a46b85678da","755133535cbb419f9e176a22c94f5e02","d18f416c9d18423bbb917cefe66d674a","1dc508cdb6b941249c2e701ee51b5bea","0c65fb773cde4f7c93a907d71350dbf6","06e36766a68c4418aff3836fac1f1698","896bbae8b78b4ac38d36638b8a29fa27","0a1e29d387ad49af98889df8d35797c0","51cf5dd55c0f4f489c4b6502a545faac"]},"executionInfo":{"status":"ok","timestamp":1711699401082,"user_tz":-60,"elapsed":11602,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"0372d231-5530-4811-f6e6-94735c4edc8c"},"outputs":[{"output_type":"stream","name":"stdout","text":["seed set to :  1000\n","Training on GPU\n","======> Success !\n","Name of GPU :  Tesla V100-SXM2-16GB\n","device is :  cuda:0\n","The model used will be :  allenai/scibert_scivocab_uncased\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93dc5767cd7243d8a3fb53509ab955b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/228k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93010b298a5d43ae80bb8f3290ab1e74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/442M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"459e1d6c79484555a99cd8bc749a22a8"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded\n"]}],"source":["# Set the model\n","\n","seed = 1000\n","lr = 0.000001351\n","weight_decay = 0.1\n","warmup_steps = 300\n","dropout_prob = 0.1\n","batch_size = 4\n","n_epochs = 3\n","class_weights = None\n","model_name = 'allenai/scibert_scivocab_uncased'\n","pu_learning = None\n","num_labels = 2\n","use_scheduler = True\n","n_gpu = 1\n","sequence_modeling = False\n","\n","assert batch_size % n_gpu == 0, \"Batch must be divisible by the number of GPUs used\"\n","\n","\n","config = {\n","            \"model_used\": model_name,\n","            \"epochs\": n_epochs,\n","            \"learning_rate\": lr,\n","            \"warmup\": warmup_steps,\n","            \"weight_decay\": weight_decay,\n","            \"batch_size\": batch_size,\n","            \"seed\": seed,\n","            \"use_scheduler\": use_scheduler,\n","            \"balance_class_weight\": class_weights,\n","            \"pu_learning\": pu_learning,\n","            \"sequence_modeling\": sequence_modeling\n","        }\n","\n","enforce_reproducibility(seed)\n","device = load_device()\n","\n","model = model_name\n","print('The model used will be : ',model)\n","print('\\n')\n","tokenizer = AutoTokenizer.from_pretrained(model)\n","tokenizer_fn = text_to_batch_transformer\n","\n","trainer= TransformerClassificationTrainer(\n","        model,\n","        device,\n","        num_labels=num_labels,\n","        tokenizer=tokenizer,\n","    )\n","\n","trainer.load('model_SBU.pth')\n","print(\"Model loaded\")"]},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FAOLDNq7H1Yq","executionInfo":{"status":"ok","timestamp":1711699403528,"user_tz":-60,"elapsed":654,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"4be3e6a4-87f1-4604-8029-b564378e336f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["annotations.tsv        model_SBU.pth       train.jsonl     twt_train.jsonl\n","Longformer_On_TWT.pth  SciBert_On_TWT.pth  twt_FULL.jsonl  val.jsonl\n","model_LFS.pth          test.jsonl          twt_test.jsonl\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t8aWblxUelsD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711699426426,"user_tz":-60,"elapsed":1166,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"736f955b-8fbc-49f9-c2ed-a7193b6b5781"},"outputs":[{"output_type":"stream","name":"stderr","text":["Processing data: 100%|██████████| 415/415 [00:00<00:00, 296682.49it/s]\n"]}],"source":["DataReaderClass = TransformerSingleSentenceDataset\n","twt_full_loc = \"twt_FULL.jsonl\"\n","twt_full = DataReaderClass(twt_full_loc, tokenizer, tokenizer_fn=tokenizer_fn)"]},{"cell_type":"code","source":["kf = KFold(n_splits=10, shuffle=True, random_state=seed)\n","\n","test_losses = []\n","accuracies = []\n","precisions = []\n","recalls = []\n","f1_scores = []\n","\n","for fold, (train_index, test_index) in enumerate(kf.split(twt_full)):\n","    print(f\"Fold {fold + 1}/{10}\")\n","    train_dset_fold = [twt_full[i] for i in train_index]\n","    test_dset_fold = [twt_full[i] for i in test_index]\n","\n","    (test_loss, acc, P, R, F1) = trainer.evaluate(test_dset_fold)\n","    test_losses.append(test_loss)\n","    accuracies.append(acc)\n","    precisions.append(P)\n","    recalls.append(R)\n","    f1_scores.append(F1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Pp94yZT57nAy","executionInfo":{"status":"ok","timestamp":1711699436942,"user_tz":-60,"elapsed":4033,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"d05a8eec-ce2f-4bf5-caa2-8f463f82690c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 1/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:02<00:00,  1.12s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 2/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 46.03it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 3/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 36.53it/s]"]},{"output_type":"stream","name":"stdout","text":["Fold 4/10\n"]},{"output_type":"stream","name":"stderr","text":["\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 48.44it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 5/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 49.32it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 6/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 48.48it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 7/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 36.91it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 8/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 41.01it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 9/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 43.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 10/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 36.44it/s]\n"]}]},{"cell_type":"code","source":["# Calculate average evaluation metrics over all folds\n","avg_test_loss = sum(test_losses) / 10\n","avg_accuracy = sum(accuracies) / 10\n","avg_precision = sum(precisions) / 10\n","avg_recall = sum(recalls) / 10\n","avg_f1_score = sum(f1_scores) / 10\n","\n","# Print average evaluation metrics\n","print('Average test-loss = ', avg_test_loss)\n","print('Average test-acc = ', avg_accuracy)\n","print()\n","print('All precisions : ',precisions)\n","print('Average test-P = ', avg_precision)\n","print()\n","print('All recalls : ',recalls)\n","print('Average test-R = ', avg_recall)\n","print()\n","print('All F1 : ',f1_scores)\n","print('Average test-F1 = ', avg_f1_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rSdyUJSCPLho","executionInfo":{"status":"ok","timestamp":1711699445665,"user_tz":-60,"elapsed":650,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"7ee7d751-590e-432f-dd0f-cd9b6bd45b90"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average test-loss =  0.832277825474739\n","Average test-acc =  0.5134146341463415\n","\n","All precisions :  [0.46153846153846156, 0.5384615384615384, 0.4, 0.6, 0.625, 0.125, 0.875, 0.75, 0.45454545454545453, 0.5]\n","Average test-P =  0.5329545454545455\n","\n","All recalls :  [0.3, 0.3181818181818182, 0.09090909090909091, 0.11538461538461539, 0.29411764705882354, 0.058823529411764705, 0.3333333333333333, 0.2608695652173913, 0.25, 0.3684210526315789]\n","Average test-R =  0.23900406521284162\n","\n","All F1 :  [0.3636363636363637, 0.39999999999999997, 0.14814814814814814, 0.1935483870967742, 0.4, 0.07999999999999999, 0.48275862068965514, 0.3870967741935483, 0.3225806451612903, 0.4242424242424242]\n","Average test-F1 =  0.32020113631682046\n"]}]},{"cell_type":"markdown","metadata":{"id":"7z7Owr96eyKj"},"source":["### Experiment 3\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qZNOxEGue7aa","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711699694744,"user_tz":-60,"elapsed":2689,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"23e6a812-fb1f-4a21-98b0-a6db90da8ecc"},"outputs":[{"output_type":"stream","name":"stdout","text":["seed set to :  1000\n","Training on GPU\n","======> Success !\n","Name of GPU :  Tesla V100-SXM2-16GB\n","device is :  cuda:0\n","The model used will be :  allenai/scibert_scivocab_uncased\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of BertForSequenceClassification were not initialized from the model checkpoint at allenai/scibert_scivocab_uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model created\n"]}],"source":["# Set the model\n","\n","seed = 1000\n","lr = 0.000001351\n","weight_decay = 0.1\n","warmup_steps = 300\n","dropout_prob = 0.1\n","batch_size = 4\n","n_epochs = 3\n","class_weights = None\n","model_name = 'allenai/scibert_scivocab_uncased'\n","pu_learning = None\n","num_labels = 2\n","use_scheduler = True\n","n_gpu = 1\n","sequence_modeling = False\n","\n","assert batch_size % n_gpu == 0, \"Batch must be divisible by the number of GPUs used\"\n","\n","\n","config = {\n","            \"model_used\": model_name,\n","            \"epochs\": n_epochs,\n","            \"learning_rate\": lr,\n","            \"warmup\": warmup_steps,\n","            \"weight_decay\": weight_decay,\n","            \"batch_size\": batch_size,\n","            \"seed\": seed,\n","            \"use_scheduler\": use_scheduler,\n","            \"balance_class_weight\": class_weights,\n","            \"pu_learning\": pu_learning,\n","            \"sequence_modeling\": sequence_modeling\n","        }\n","\n","enforce_reproducibility(seed)\n","device = load_device()\n","\n","model = model_name\n","print('The model used will be : ',model)\n","print('\\n')\n","tokenizer = AutoTokenizer.from_pretrained(model)\n","tokenizer_fn = text_to_batch_transformer\n","\n","trainer= TransformerClassificationTrainer(\n","        model,\n","        device,\n","        num_labels=num_labels,\n","        tokenizer=tokenizer,\n","    )\n","print(\"Model created\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bTOfiZUVfB1e","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711699698978,"user_tz":-60,"elapsed":315,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"25359827-2506-45ff-f09b-ab40483ec6cc"},"outputs":[{"output_type":"stream","name":"stderr","text":["Processing data: 100%|██████████| 415/415 [00:00<00:00, 584498.37it/s]\n"]}],"source":["DataReaderClass = TransformerSingleSentenceDataset\n","twt_full_loc = \"twt_FULL.jsonl\"\n","twt_full = DataReaderClass(twt_full_loc, tokenizer, tokenizer_fn=tokenizer_fn)"]},{"cell_type":"code","source":["kf = KFold(n_splits=10, random_state=seed, shuffle=True)\n","\n","test_losses = []\n","accuracies = []\n","precisions = []\n","recalls = []\n","f1_scores = []\n","\n","# Perform cross-validation\n","for fold_num, (train_index, test_index) in enumerate(kf.split(twt_full), start=1):\n","    # Split the data into training and test sets for this fold\n","    train_dset_fold = [twt_full[i] for i in train_index]\n","    test_dset_fold = [twt_full[i] for i in test_index]\n","\n","    #create a vlaidation subsets :\n","    train_count = int(len(train_dset_fold) * 0.88) # 88% de train\n","    val_count = len(train_dset_fold) - train_count # 30% de val\n","    random.seed(seed) # mettre le seed pour pouvoir reproduire\n","    indices_train = random.sample(range(len(train_dset_fold)), train_count)\n","\n","    train_dset = [train_dset_fold[i] for i in range(len(train_dset_fold)) if i in indices_train]\n","    val_dset = [train_dset_fold[i] for i in range(len(train_dset_fold)) if i not in indices_train]\n","    # Train the model\n","    trainer.train(\n","          train_dset,\n","          val_dset,\n","          weight_decay=weight_decay,\n","          model_file=\"SciBert_On_TWT.pth\",\n","          class_weights=class_weights,\n","          metric_name='F1',\n","          logger=None,\n","          lr=lr,\n","          warmup_steps=warmup_steps,\n","          n_epochs=n_epochs,\n","          batch_size=batch_size,\n","          use_scheduler=use_scheduler\n","      )\n","    print(\"###########################################################################################################################\")\n","    print(f\"\\r\\r\\r\\r\\r Fold {fold_num} completed.\")\n","    print(\"###########################################################################################################################\")\n","\n","    # Evaluate the model on the test set\n","    test_loss, acc, P, R, F1 = trainer.evaluate(test_dset_fold)\n","\n","    # Record evaluation metrics for this fold\n","    test_losses.append(test_loss)\n","    accuracies.append(acc)\n","    precisions.append(P)\n","    recalls.append(R)\n","    f1_scores.append(F1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w-RnOTi5P7Es","outputId":"3c6ecc61-8862-4842-d47d-0b5289c4ced8","executionInfo":{"status":"ok","timestamp":1711699884039,"user_tz":-60,"elapsed":183033,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 19.05it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 40.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.37209302325581395\n","{'Validation accuracy': 0.4, 'Validation Precision': 0.5333333333333333, 'Validation Recall': 0.2857142857142857, 'Validation F1': 0.37209302325581395, 'Validation loss': 0.7251751124858856}\n","Best F1: 0.37209302325581395\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 19.20it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 41.21it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.46808510638297873\n","{'Validation accuracy': 0.4444444444444444, 'Validation Precision': 0.5789473684210527, 'Validation Recall': 0.39285714285714285, 'Validation F1': 0.46808510638297873, 'Validation loss': 0.7154655158519745}\n","Best F1: 0.46808510638297873\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 19.79it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 40.10it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.5306122448979592\n","{'Validation accuracy': 0.4888888888888889, 'Validation Precision': 0.6190476190476191, 'Validation Recall': 0.4642857142857143, 'Validation F1': 0.5306122448979592, 'Validation loss': 0.6922200620174408}\n","Best F1: 0.5306122448979592\n","###########################################################################################################################\n"," Fold 1 completed.\n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 36.83it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 18.70it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 36.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.6\n","{'Validation accuracy': 0.6444444444444445, 'Validation Precision': 0.7058823529411765, 'Validation Recall': 0.5217391304347826, 'Validation F1': 0.6, 'Validation loss': 0.6581875681877136}\n","Best F1: 0.6\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 19.73it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 37.46it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.6341463414634146\n","{'Validation accuracy': 0.6666666666666666, 'Validation Precision': 0.7222222222222222, 'Validation Recall': 0.5652173913043478, 'Validation F1': 0.6341463414634146, 'Validation loss': 0.6510366797447205}\n","Best F1: 0.6341463414634146\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 19.55it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 36.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.6153846153846153\n","{'Validation accuracy': 0.6666666666666666, 'Validation Precision': 0.75, 'Validation Recall': 0.5217391304347826, 'Validation F1': 0.6153846153846153, 'Validation loss': 0.6381196677684784}\n","###########################################################################################################################\n"," Fold 2 completed.\n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 50.84it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 18.04it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 35.38it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.7441860465116279\n","{'Validation accuracy': 0.7555555555555555, 'Validation Precision': 0.8, 'Validation Recall': 0.6956521739130435, 'Validation F1': 0.7441860465116279, 'Validation loss': 0.6129969656467438}\n","Best F1: 0.7441860465116279\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 19.31it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 36.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.7272727272727272\n","{'Validation accuracy': 0.7333333333333333, 'Validation Precision': 0.7619047619047619, 'Validation Recall': 0.6956521739130435, 'Validation F1': 0.7272727272727272, 'Validation loss': 0.6020036935806274}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 20.09it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 35.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.7916666666666667\n","{'Validation accuracy': 0.7777777777777778, 'Validation Precision': 0.76, 'Validation Recall': 0.8260869565217391, 'Validation F1': 0.7916666666666667, 'Validation loss': 0.5783651173114777}\n","Best F1: 0.7916666666666667\n","###########################################################################################################################\n"," Fold 3 completed.\n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 40.32it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 19.51it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 46.79it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.7391304347826089\n","{'Validation accuracy': 0.7333333333333333, 'Validation Precision': 0.8095238095238095, 'Validation Recall': 0.68, 'Validation F1': 0.7391304347826089, 'Validation loss': 0.5616118907928467}\n","Best F1: 0.7391304347826089\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 19.98it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 48.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.7111111111111111\n","{'Validation accuracy': 0.7111111111111111, 'Validation Precision': 0.8, 'Validation Recall': 0.64, 'Validation F1': 0.7111111111111111, 'Validation loss': 0.5574941337108612}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 18.17it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 49.13it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.7111111111111111\n","{'Validation accuracy': 0.7111111111111111, 'Validation Precision': 0.8, 'Validation Recall': 0.64, 'Validation F1': 0.7111111111111111, 'Validation loss': 0.5514318943023682}\n","###########################################################################################################################\n"," Fold 4 completed.\n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 52.47it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 18.10it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 41.12it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.6521739130434783\n","{'Validation accuracy': 0.6444444444444445, 'Validation Precision': 0.6818181818181818, 'Validation Recall': 0.625, 'Validation F1': 0.6521739130434783, 'Validation loss': 0.5996938347816467}\n","Best F1: 0.6521739130434783\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 19.38it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 41.26it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.6938775510204083\n","{'Validation accuracy': 0.6666666666666666, 'Validation Precision': 0.68, 'Validation Recall': 0.7083333333333334, 'Validation F1': 0.6938775510204083, 'Validation loss': 0.5893525183200836}\n","Best F1: 0.6938775510204083\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:04<00:00, 19.25it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 40.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.6808510638297872\n","{'Validation accuracy': 0.6666666666666666, 'Validation Precision': 0.6956521739130435, 'Validation Recall': 0.6666666666666666, 'Validation F1': 0.6808510638297872, 'Validation loss': 0.5713020861148834}\n","###########################################################################################################################\n"," Fold 5 completed.\n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 54.46it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 19.08it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 46.20it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.8235294117647058\n","{'Validation accuracy': 0.8, 'Validation Precision': 0.8076923076923077, 'Validation Recall': 0.84, 'Validation F1': 0.8235294117647058, 'Validation loss': 0.5326987504959106}\n","Best F1: 0.8235294117647058\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 19.32it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 45.70it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.8235294117647058\n","{'Validation accuracy': 0.8, 'Validation Precision': 0.8076923076923077, 'Validation Recall': 0.84, 'Validation F1': 0.8235294117647058, 'Validation loss': 0.519924134016037}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 18.18it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 45.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.8235294117647058\n","{'Validation accuracy': 0.8, 'Validation Precision': 0.8076923076923077, 'Validation Recall': 0.84, 'Validation F1': 0.8235294117647058, 'Validation loss': 0.49662207067012787}\n","###########################################################################################################################\n"," Fold 6 completed.\n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 51.35it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 18.76it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 46.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.8095238095238095\n","{'Validation accuracy': 0.8222222222222222, 'Validation Precision': 0.8947368421052632, 'Validation Recall': 0.7391304347826086, 'Validation F1': 0.8095238095238095, 'Validation loss': 0.4754879027605057}\n","Best F1: 0.8095238095238095\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 19.65it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 47.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.8095238095238095\n","{'Validation accuracy': 0.8222222222222222, 'Validation Precision': 0.8947368421052632, 'Validation Recall': 0.7391304347826086, 'Validation F1': 0.8095238095238095, 'Validation loss': 0.4642612934112549}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 19.77it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 46.95it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.8181818181818182\n","{'Validation accuracy': 0.8222222222222222, 'Validation Precision': 0.8571428571428571, 'Validation Recall': 0.782608695652174, 'Validation F1': 0.8181818181818182, 'Validation loss': 0.44534194469451904}\n","Best F1: 0.8181818181818182\n","###########################################################################################################################\n"," Fold 7 completed.\n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 39.81it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 19.85it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 49.07it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.816326530612245\n","{'Validation accuracy': 0.8, 'Validation Precision': 0.8333333333333334, 'Validation Recall': 0.8, 'Validation F1': 0.816326530612245, 'Validation loss': 0.4454493075609207}\n","Best F1: 0.816326530612245\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 20.35it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 47.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.8085106382978724\n","{'Validation accuracy': 0.8, 'Validation Precision': 0.8636363636363636, 'Validation Recall': 0.76, 'Validation F1': 0.8085106382978724, 'Validation loss': 0.43376173079013824}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 18.49it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 49.17it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.816326530612245\n","{'Validation accuracy': 0.8, 'Validation Precision': 0.8333333333333334, 'Validation Recall': 0.8, 'Validation F1': 0.816326530612245, 'Validation loss': 0.41543152928352356}\n","###########################################################################################################################\n"," Fold 8 completed.\n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 44.56it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 18.83it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 42.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.7906976744186046\n","{'Validation accuracy': 0.8, 'Validation Precision': 0.8947368421052632, 'Validation Recall': 0.7083333333333334, 'Validation F1': 0.7906976744186046, 'Validation loss': 0.508921667933464}\n","Best F1: 0.7906976744186046\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 19.80it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 42.85it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.7727272727272727\n","{'Validation accuracy': 0.7777777777777778, 'Validation Precision': 0.85, 'Validation Recall': 0.7083333333333334, 'Validation F1': 0.7727272727272727, 'Validation loss': 0.50067999958992}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 18.89it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 41.97it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.8095238095238096\n","{'Validation accuracy': 0.8222222222222222, 'Validation Precision': 0.9444444444444444, 'Validation Recall': 0.7083333333333334, 'Validation F1': 0.8095238095238096, 'Validation loss': 0.48233428597450256}\n","Best F1: 0.8095238095238096\n","###########################################################################################################################\n"," Fold 9 completed.\n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 47.41it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 19.11it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 49.18it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.7317073170731706\n","{'Validation accuracy': 0.7555555555555555, 'Validation Precision': 0.75, 'Validation Recall': 0.7142857142857143, 'Validation F1': 0.7317073170731706, 'Validation loss': 0.512921929359436}\n","Best F1: 0.7317073170731706\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 19.18it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 39.08it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.7317073170731706\n","{'Validation accuracy': 0.7555555555555555, 'Validation Precision': 0.75, 'Validation Recall': 0.7142857142857143, 'Validation F1': 0.7317073170731706, 'Validation loss': 0.5104428976774216}\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:04<00:00, 18.61it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00, 47.77it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.6486486486486486\n","{'Validation accuracy': 0.7111111111111111, 'Validation Precision': 0.75, 'Validation Recall': 0.5714285714285714, 'Validation F1': 0.6486486486486486, 'Validation loss': 0.5144435465335846}\n","###########################################################################################################################\n"," Fold 10 completed.\n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00, 40.20it/s]\n"]}]},{"cell_type":"code","source":["# Calculate average evaluation metrics over all folds\n","avg_test_loss = sum(test_losses) / 10\n","avg_accuracy = sum(accuracies) / 10\n","avg_precision = sum(precisions) / 10\n","avg_recall = sum(recalls) / 10\n","avg_f1_score = sum(f1_scores) / 10\n","\n","# Print average evaluation metrics\n","print('Average test-loss = ', avg_test_loss)\n","print('Average test-acc = ', avg_accuracy)\n","print()\n","print('All precisions : ',precisions)\n","print('Average test-P = ', avg_precision)\n","print()\n","print('All recalls : ',recalls)\n","print('Average test-R = ', avg_recall)\n","print()\n","print('All F1 : ',f1_scores)\n","print('Average test-F1 = ', avg_f1_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ge7jIqKGP7AV","executionInfo":{"status":"ok","timestamp":1711699893409,"user_tz":-60,"elapsed":650,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"537b77bc-438d-4db9-d90d-46d736f838e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average test-loss =  0.5492333129048348\n","Average test-acc =  0.7522067363530777\n","\n","All precisions :  [0.5333333333333333, 0.7777777777777778, 0.8571428571428571, 0.8461538461538461, 0.7368421052631579, 0.6, 0.7391304347826086, 0.85, 0.9411764705882353, 0.8095238095238095]\n","Average test-P =  0.7691080634565626\n","\n","All recalls :  [0.4, 0.6363636363636364, 0.5454545454545454, 0.8461538461538461, 0.8235294117647058, 0.5294117647058824, 0.8095238095238095, 0.7391304347826086, 0.8, 0.8947368421052632]\n","Average test-R =  0.7024304290854296\n","\n","All F1 :  [0.4571428571428572, 0.7000000000000001, 0.6666666666666665, 0.8461538461538461, 0.7777777777777778, 0.5625, 0.7727272727272727, 0.7906976744186046, 0.8648648648648648, 0.8500000000000001]\n","Average test-F1 =  0.728853095975189\n"]}]},{"cell_type":"markdown","source":["## Longformer"],"metadata":{"id":"HqflRdBi8rnf"}},{"cell_type":"markdown","metadata":{"id":"VO45b5vWDChB"},"source":["### Experiment 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"guYBJZLdDChC"},"outputs":[],"source":["#run = 'SBU'\n","run = 'LFsolo'\n","\n","# Setting Parameters and random seed\n","if run == 'SBU' :\n","  seed = 1000\n","  lr = 0.000001351\n","  weight_decay = 0.1\n","  warmup_steps = 300\n","  dropout_prob = 0.1\n","  batch_size = 4\n","  n_epochs = 3\n","  class_weights = None\n","  model_name = 'allenai/scibert_scivocab_uncased'\n","  pu_learning = None\n","  num_labels = 2\n","  use_scheduler = True\n","  n_gpu = 1\n","  sequence_modeling = False\n","else :\n","  seed = 1000\n","  lr = 0.000001351\n","  weight_decay = 0.1\n","  warmup_steps = 300\n","  dropout_prob = 0.1\n","  batch_size = 4\n","  n_epochs = 3\n","  class_weights = 'balanced'\n","  model_name = 'allenai/longformer-base-4096'\n","  pu_learning = None\n","  num_labels = 2\n","  use_scheduler = True\n","  n_gpu = 1\n","  sequence_modeling = False\n","\n","\n","\n","assert batch_size % n_gpu == 0, \"Batch must be divisible by the number of GPUs used\"\n","\n","\n","config = {\n","            \"model_used\": model_name,\n","            \"epochs\": n_epochs,\n","            \"learning_rate\": lr,\n","            \"warmup\": warmup_steps,\n","            \"weight_decay\": weight_decay,\n","            \"batch_size\": batch_size,\n","            \"seed\": seed,\n","            \"use_scheduler\": use_scheduler,\n","            \"balance_class_weight\": class_weights,\n","            \"pu_learning\": pu_learning,\n","            \"sequence_modeling\": sequence_modeling\n","        }\n","\n","enforce_reproducibility(seed)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C0xgoHHrDChD"},"outputs":[],"source":["device = load_device()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDUFZOTODChD"},"outputs":[],"source":["# Getting the data\n","\n","train_data_loc = \"train.jsonl\"\n","valid_data_loc = \"val.jsonl\"\n","test_data_loc = \"test.jsonl\"\n","\n","\n","# Initiating the model, Tokenizer and tokenizer function\n","if sequence_modeling :\n","  DataReaderClass = TransformerMultiSentenceDataset\n","  tokenizer = AutoTokenizer.from_pretrained(model_name)\n","  tokenizer_fn = text_to_sequence_batch_transformer\n","  model = AutoTransformerForSentenceSequenceModeling(\n","            model_name,\n","            num_labels=num_labels,\n","            sep_token_id=tokenizer.sep_token_id\n","        ).to(device)\n","  print('The model used will be : ',model_name)\n","  print('\\n')\n","else :\n","  DataReaderClass = TransformerSingleSentenceDataset\n","  model = model_name\n","  print('The model used will be : ',model)\n","  print('\\n')\n","  tokenizer = AutoTokenizer.from_pretrained(model)\n","  tokenizer_fn = text_to_batch_transformer\n","\n","\n","\n","# Formatting the data and tokenizing it using the class and tokenizer function\n","train_dset = DataReaderClass(train_data_loc, tokenizer, tokenizer_fn=tokenizer_fn)\n","valid_dset = DataReaderClass(valid_data_loc, tokenizer, tokenizer_fn=tokenizer_fn)\n","test_dset = DataReaderClass(test_data_loc, tokenizer, tokenizer_fn=tokenizer_fn)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wHIm557lDChD"},"outputs":[],"source":["# Creating the training class\n","trainer = TransformerClassificationTrainer(\n","        model,\n","        device,\n","        num_labels=num_labels,\n","        tokenizer=tokenizer\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Jkx119ADChD"},"outputs":[],"source":["check_attributes(trainer, config, DataReaderClass)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8ZCSD2CDDChE"},"outputs":[],"source":["# Training the model\n","trainer.train(\n","        train_dset,\n","        valid_dset,\n","        weight_decay=weight_decay,\n","        model_file=\"model_LongFormer.pth\",\n","        class_weights=class_weights,\n","        metric_name='F1',\n","        logger=None,\n","        lr=lr,\n","        warmup_steps=warmup_steps,\n","        n_epochs=n_epochs,\n","        batch_size=batch_size,\n","        use_scheduler=use_scheduler\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tro1N9F7DChE"},"outputs":[],"source":["(test_loss, acc, P, R, F1) = trainer.evaluate(test_dset)\n","\n","print('test-loss = ', test_loss)\n","print('test-acc = ', acc)\n","print('test-P = ', P)\n","print('test-R = ', R)\n","print('test-F1 = ', F1)\n"]},{"cell_type":"markdown","metadata":{"id":"VjMqUaJvfdjj"},"source":["### Experiment 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9C8FAEQSfkKN","colab":{"base_uri":"https://localhost:8080/","height":388,"referenced_widgets":["6d0daa3359fa4776a72499bf6ac569b1","3b2ffdfc221445c0a816e25e64d8ac56","75acab21c1aa407e873e91821ff1d76d","bd3cc6020caa42efab79b1ec9fd8287d","8c9b86848df6449f9e8885474e067445","f1527e14b7964894a8317d766ef34a08","0b93ee1964ba44468e8a67a67fb46992","3661533b16b747d9ad4e54b1a0d49411","d8120080864c465a87440d7cb3f0807e","0950e06f1d9442a48be082346c8dfa6a","aef75eaa33db48ac891b61dd88c57470","5e59c2f0d8c5410e98fc32e1ff8caca3","e1ce5d8567254d2296f92a0d6e151893","3b318a830b0a4ba9b08020960116f35c","94ee0b38df2f49dbb31631d9856f0477","ec5515f54df84ceaa43a8de76b8cd052","63e9f5a3f4e7448383642fb83e5720db","836ce6f4d11a464a93945a5f5273746e","ea13c88216054470ab115037fb3e250d","6f12e9d5955b4a1689ed1f27542194f2","c3f336b1618948e5a8770951605ce267","94d84a1fb97d4fd1a12942ddc965aa71","a3aa4654438048d797130812b6591873","eb21b328ace4417a9d888554fa73f1b3","04364746588747268e677e46e73c96e6","190425f648d541dcbd4f8f3a9edfb6a5","68091ae0bb5c4dd49f64fbeba41ca81a","a9645d296c954557b97630d39191fb46","3c038d66ddab43f5a5161fc63f2e0030","eb6bacbb19d24f00b025d13874d3c387","49af9a7429ba48a9aece4567decab847","fa99ee51206f4d9abbdc6a4486e5d789","9b6e6fc16952420f9ff378822af302e9","06f4b39ca8fa4491bc71ec23783360b7","0a19050189504bb398c11db760b80cd7","b991414540e2463cbdb4cbbb54907b84","a0b78d2b4f6a4cfdb56659fd247b3f23","f8d561e942624d9ab529b9be9b07216b","db8214f094f044ee9c8b7ce870a80f01","ecd76a9456f444f0877e37fac023a55d","9fcc7ba7f82a49e3b6f2497c2d70213c","6f382a34d4d545d384717fbb6515f8d3","7d223a7594e84aa29961b19ad9b9060f","142761b6a3524762ae77e0179d2f21fa","3c999d6415604500b8ad52f31704a934","d2804fa5a9e7459e90e81c5224d952b5","f9cac0759dc148cdb4a45a5dce840cef","288a0bb320744111b3c000ab84319fc5","d8a1f6e43d364c50bc4abc778cdbd8fc","0974544f750c4985bf75771201ff91b5","bb0013213e944639be0cc6e6db225b90","4b48487ea68046fd9fc4858f476fb495","213bdfd8649441488365a403ff0761cc","2036e4bd93e54f098c81992c519ad8b1","54e28b201d2842108f76beee62d1cabe"]},"executionInfo":{"status":"ok","timestamp":1711700073934,"user_tz":-60,"elapsed":10867,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"3d27373e-5582-4351-d69b-6fa5bc1ed01a"},"outputs":[{"output_type":"stream","name":"stdout","text":["seed set to :  1000\n","Training on GPU\n","======> Success !\n","Name of GPU :  Tesla V100-SXM2-16GB\n","device is :  cuda:0\n","The model used will be :  allenai/longformer-base-4096\n","\n","\n"]},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d0daa3359fa4776a72499bf6ac569b1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e59c2f0d8c5410e98fc32e1ff8caca3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3aa4654438048d797130812b6591873"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f4b39ca8fa4491bc71ec23783360b7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["pytorch_model.bin:   0%|          | 0.00/597M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3c999d6415604500b8ad52f31704a934"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model loaded\n"]}],"source":["seed = 1000\n","lr = 0.000001351\n","weight_decay = 0.1\n","warmup_steps = 300\n","dropout_prob = 0.1\n","batch_size = 4\n","n_epochs = 3\n","class_weights = 'balanced'\n","model_name = 'allenai/longformer-base-4096'\n","pu_learning = None\n","num_labels = 2\n","use_scheduler = True\n","n_gpu = 1\n","sequence_modeling = False\n","\n","assert batch_size % n_gpu == 0, \"Batch must be divisible by the number of GPUs used\"\n","\n","\n","config = {\n","            \"model_used\": model_name,\n","            \"epochs\": n_epochs,\n","            \"learning_rate\": lr,\n","            \"warmup\": warmup_steps,\n","            \"weight_decay\": weight_decay,\n","            \"batch_size\": batch_size,\n","            \"seed\": seed,\n","            \"use_scheduler\": use_scheduler,\n","            \"balance_class_weight\": class_weights,\n","            \"pu_learning\": pu_learning,\n","            \"sequence_modeling\": sequence_modeling\n","        }\n","\n","enforce_reproducibility(seed)\n","device = load_device()\n","\n","model = model_name\n","print('The model used will be : ',model)\n","print('\\n')\n","tokenizer = AutoTokenizer.from_pretrained(model)\n","tokenizer_fn = text_to_batch_transformer\n","\n","trainer= TransformerClassificationTrainer(\n","        model,\n","        device,\n","        num_labels=num_labels,\n","        tokenizer=tokenizer,\n","    )\n","\n","trainer.load('model_LFS.pth')\n","print(\"Model loaded\")"]},{"cell_type":"code","source":["%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tYFB1q1oz05_","executionInfo":{"status":"ok","timestamp":1711700075810,"user_tz":-60,"elapsed":429,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"18504098-8402-4220-f285-01998fdf2b3e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["annotations.tsv        model_SBU.pth       train.jsonl     twt_train.jsonl\n","Longformer_On_TWT.pth  SciBert_On_TWT.pth  twt_FULL.jsonl  val.jsonl\n","model_LFS.pth          test.jsonl          twt_test.jsonl\n"]}]},{"cell_type":"code","source":["DataReaderClass = TransformerSingleSentenceDataset\n","twt_full_loc = \"twt_FULL.jsonl\"\n","twt_full = DataReaderClass(twt_full_loc, tokenizer, tokenizer_fn=tokenizer_fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YlLSSjxJz-Pj","executionInfo":{"status":"ok","timestamp":1711700080942,"user_tz":-60,"elapsed":2,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"f68fa842-1e79-4a43-b4cb-020f2f517538"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing data: 100%|██████████| 415/415 [00:00<00:00, 707863.42it/s]\n"]}]},{"cell_type":"code","source":["kf = KFold(n_splits=10, shuffle=True, random_state=seed)\n","\n","test_losses = []\n","accuracies = []\n","precisions = []\n","recalls = []\n","f1_scores = []\n","\n","for fold, (train_index, test_index) in enumerate(kf.split(twt_full)):\n","    print(f\"Fold {fold + 1}/{10}\")\n","    train_dset_fold = [twt_full[i] for i in train_index]\n","    test_dset_fold = [twt_full[i] for i in test_index]\n","\n","    (test_loss, acc, P, R, F1) = trainer.evaluate(test_dset_fold)\n","    test_losses.append(test_loss)\n","    accuracies.append(acc)\n","    precisions.append(P)\n","    recalls.append(R)\n","    f1_scores.append(F1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DN6KsqeH17uk","executionInfo":{"status":"ok","timestamp":1711700096248,"user_tz":-60,"elapsed":9318,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"747dfb0b-70db-4ad7-9aba-64bcc2fb3ed0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fold 1/10\n"]},{"output_type":"stream","name":"stderr","text":["\rEvaluation:   0%|          | 0/2 [00:00<?, ?it/s]Initializing global attention on CLS token...\n","Input ids are automatically padded from 68 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation:  50%|█████     | 1/2 [00:00<00:00,  1.10it/s]Input ids are automatically padded from 59 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation: 100%|██████████| 2/2 [00:01<00:00,  1.82it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 2/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation:   0%|          | 0/2 [00:00<?, ?it/s]Input ids are automatically padded from 45 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation:  50%|█████     | 1/2 [00:00<00:00,  1.79it/s]Input ids are automatically padded from 60 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 3/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation:   0%|          | 0/2 [00:00<?, ?it/s]Input ids are automatically padded from 61 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation:  50%|█████     | 1/2 [00:00<00:00,  1.78it/s]Input ids are automatically padded from 65 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 4/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation:   0%|          | 0/2 [00:00<?, ?it/s]Input ids are automatically padded from 53 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation:  50%|█████     | 1/2 [00:00<00:00,  1.80it/s]Input ids are automatically padded from 55 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.68it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 5/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation:   0%|          | 0/2 [00:00<?, ?it/s]Input ids are automatically padded from 39 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation:  50%|█████     | 1/2 [00:00<00:00,  1.79it/s]Input ids are automatically padded from 69 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.66it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 6/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation:   0%|          | 0/2 [00:00<?, ?it/s]Input ids are automatically padded from 77 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 7/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation:  50%|█████     | 1/2 [00:00<00:00,  1.79it/s]Input ids are automatically padded from 64 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.72it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 8/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation:   0%|          | 0/2 [00:00<?, ?it/s]Input ids are automatically padded from 58 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 9/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation:   0%|          | 0/2 [00:00<?, ?it/s]Input ids are automatically padded from 50 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Fold 10/10\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation:   0%|          | 0/2 [00:00<?, ?it/s]Input ids are automatically padded from 62 to 512 to be a multiple of `config.attention_window`: 512\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.72it/s]\n"]}]},{"cell_type":"code","source":["# Calculate average evaluation metrics over all folds\n","avg_test_loss = sum(test_losses) / 10\n","avg_accuracy = sum(accuracies) / 10\n","avg_precision = sum(precisions) / 10\n","avg_recall = sum(recalls) / 10\n","avg_f1_score = sum(f1_scores) / 10\n","\n","# Print average evaluation metrics\n","print('Average test-loss = ', avg_test_loss)\n","print('Average test-acc = ', avg_accuracy)\n","print()\n","print('All precisions : ',precisions)\n","print('Average test-P = ', avg_precision)\n","print()\n","print('All recalls : ',recalls)\n","print('Average test-R = ', avg_recall)\n","print()\n","print('All F1 : ',f1_scores)\n","print('Average test-F1 = ', avg_f1_score)"],"metadata":{"id":"L527fefh5kYe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711700099773,"user_tz":-60,"elapsed":556,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"058e6cf7-8920-4351-d8f7-b221557e1664"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average test-loss =  1.4004081398248673\n","Average test-acc =  0.5205574912891986\n","\n","All precisions :  [0.4375, 0.5454545454545454, 0.2, 0.8571428571428571, 0.7, 0.3333333333333333, 0.7777777777777778, 0.75, 0.5, 0.3333333333333333]\n","Average test-P =  0.5434541847041847\n","\n","All recalls :  [0.35, 0.2727272727272727, 0.045454545454545456, 0.23076923076923078, 0.4117647058823529, 0.17647058823529413, 0.3333333333333333, 0.2608695652173913, 0.15, 0.15789473684210525]\n","Average test-R =  0.23892839784615255\n","\n","All F1 :  [0.38888888888888884, 0.3636363636363636, 0.07407407407407407, 0.36363636363636365, 0.5185185185185185, 0.23076923076923078, 0.4666666666666666, 0.3870967741935483, 0.23076923076923075, 0.21428571428571427]\n","Average test-F1 =  0.32383418254386\n"]}]},{"cell_type":"markdown","metadata":{"id":"OKsDO7SGfdaX"},"source":["### Experiment 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xPlA0FkKfkjj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1711700237820,"user_tz":-60,"elapsed":2289,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"d7c6d63c-97e1-4464-b2bc-51701e9c5b69"},"outputs":[{"output_type":"stream","name":"stdout","text":["seed set to :  1000\n","Training on GPU\n","======> Success !\n","Name of GPU :  Tesla V100-SXM2-16GB\n","device is :  cuda:0\n","The model used will be :  allenai/longformer-base-4096\n","\n","\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Model created\n"]}],"source":["seed = 1000\n","lr = 0.000001351\n","weight_decay = 0.1\n","warmup_steps = 300\n","dropout_prob = 0.1\n","batch_size = 4\n","n_epochs = 3\n","class_weights = 'balanced'\n","model_name = 'allenai/longformer-base-4096'\n","pu_learning = None\n","num_labels = 2\n","use_scheduler = True\n","n_gpu = 1\n","sequence_modeling = False\n","\n","assert batch_size % n_gpu == 0, \"Batch must be divisible by the number of GPUs used\"\n","\n","config = {\n","            \"model_used\": model_name,\n","            \"epochs\": n_epochs,\n","            \"learning_rate\": lr,\n","            \"warmup\": warmup_steps,\n","            \"weight_decay\": weight_decay,\n","            \"batch_size\": batch_size,\n","            \"seed\": seed,\n","            \"use_scheduler\": use_scheduler,\n","            \"balance_class_weight\": class_weights,\n","            \"pu_learning\": pu_learning,\n","            \"sequence_modeling\": sequence_modeling\n","        }\n","\n","enforce_reproducibility(seed)\n","device = load_device()\n","\n","model = model_name\n","print('The model used will be : ',model)\n","print('\\n')\n","tokenizer = AutoTokenizer.from_pretrained(model)\n","tokenizer_fn = text_to_batch_transformer\n","\n","trainer= TransformerClassificationTrainer(\n","        model,\n","        device,\n","        num_labels=num_labels,\n","        tokenizer=tokenizer,\n","    )\n","print(\"Model created\")"]},{"cell_type":"code","source":["DataReaderClass = TransformerSingleSentenceDataset\n","twt_full_loc = \"twt_FULL.jsonl\"\n","twt_full = DataReaderClass(twt_full_loc, tokenizer, tokenizer_fn=tokenizer_fn)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xa5cFycaC1kF","executionInfo":{"status":"ok","timestamp":1711700245718,"user_tz":-60,"elapsed":501,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"09e8b8d6-f9e0-4340-9252-c4639798dd54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Processing data: 100%|██████████| 415/415 [00:00<00:00, 867714.94it/s]\n"]}]},{"cell_type":"code","source":["#import warnings\n","#warnings.filterwarnings('always')  # \"error\", \"ignore\", \"always\", \"default\", \"module\" or \"once\"\n","\n","kf = KFold(n_splits=10, random_state=seed, shuffle=True)\n","\n","test_losses = []\n","accuracies = []\n","precisions = []\n","recalls = []\n","f1_scores = []\n","\n","# Perform cross-validation\n","for fold_num, (train_index, test_index) in enumerate(kf.split(twt_full), start=1):\n","    print(\"###########################################################################################################################\")\n","    print(f\"\\r\\r\\r\\r\\r Fold {fold_num}/{10} \")\n","    print(\"###########################################################################################################################\")\n","    # Split the data into training and validation sets for this fold\n","    train_dset_fold = [twt_full[i] for i in train_index]\n","    test_dset_fold = [twt_full[i] for i in test_index]\n","\n","    #create a vlaidation subsets :\n","    train_count = int(len(train_dset_fold) * 0.88) # 88% de train\n","    random.seed(seed) # mettre le seed pour pouvoir reproduire\n","    indices_train = random.sample(range(len(train_dset_fold)), train_count)\n","\n","    train_dset = [train_dset_fold[i] for i in range(len(train_dset_fold)) if i in indices_train]\n","    val_dset = [train_dset_fold[i] for i in range(len(train_dset_fold)) if i not in indices_train]\n","\n","    # Train the model\n","    trainer.train(\n","        train_dset,\n","        val_dset,\n","        weight_decay=weight_decay,\n","        model_file=\"Longformer_On_TWT.pth\",\n","        class_weights=class_weights,\n","        metric_name='F1',\n","        logger=None,\n","        lr=lr,\n","        warmup_steps=warmup_steps,\n","        n_epochs=n_epochs,\n","        batch_size=batch_size,\n","        use_scheduler=use_scheduler\n","    )\n","\n","\n","    # Evaluate the model on the test set\n","    test_loss, acc, P, R, F1 = trainer.evaluate(test_dset_fold)\n","\n","    # Record evaluation metrics for this fold\n","    test_losses.append(test_loss)\n","    accuracies.append(acc)\n","    precisions.append(P)\n","    recalls.append(R)\n","    f1_scores.append(F1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DfKwMpGiC16L","executionInfo":{"status":"ok","timestamp":1711701218723,"user_tz":-60,"elapsed":936111,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"9bcd55dd-bcd6-4c5f-b339-0f1122f5440c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["###########################################################################################################################\n","\r\r\r\r\r Fold 1/10 \n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training:   0%|          | 0/82 [00:00<?, ?it/s]Input ids are automatically padded from 42 to 512 to be a multiple of `config.attention_window`: 512\n","Training:   1%|          | 1/82 [00:00<00:45,  1.78it/s]Input ids are automatically padded from 44 to 512 to be a multiple of `config.attention_window`: 512\n","Training:   2%|▏         | 2/82 [00:00<00:34,  2.31it/s]Input ids are automatically padded from 35 to 512 to be a multiple of `config.attention_window`: 512\n","Training:   4%|▎         | 3/82 [00:01<00:30,  2.56it/s]Input ids are automatically padded from 24 to 512 to be a multiple of `config.attention_window`: 512\n","Training:   7%|▋         | 6/82 [00:02<00:26,  2.83it/s]Input ids are automatically padded from 51 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  13%|█▎        | 11/82 [00:03<00:24,  2.91it/s]Input ids are automatically padded from 34 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  15%|█▍        | 12/82 [00:04<00:23,  2.92it/s]Input ids are automatically padded from 23 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  16%|█▌        | 13/82 [00:04<00:23,  2.92it/s]Input ids are automatically padded from 26 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  17%|█▋        | 14/82 [00:04<00:23,  2.93it/s]Input ids are automatically padded from 48 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  18%|█▊        | 15/82 [00:05<00:22,  2.93it/s]Input ids are automatically padded from 36 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  22%|██▏       | 18/82 [00:06<00:21,  2.94it/s]Input ids are automatically padded from 56 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  23%|██▎       | 19/82 [00:06<00:21,  2.94it/s]Input ids are automatically padded from 46 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  24%|██▍       | 20/82 [00:07<00:21,  2.93it/s]Input ids are automatically padded from 25 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  26%|██▌       | 21/82 [00:07<00:20,  2.93it/s]Input ids are automatically padded from 29 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  27%|██▋       | 22/82 [00:07<00:20,  2.93it/s]Input ids are automatically padded from 32 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  30%|███       | 25/82 [00:08<00:19,  2.94it/s]Input ids are automatically padded from 22 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  32%|███▏      | 26/82 [00:09<00:19,  2.94it/s]Input ids are automatically padded from 63 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  33%|███▎      | 27/82 [00:09<00:18,  2.92it/s]Input ids are automatically padded from 40 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  34%|███▍      | 28/82 [00:09<00:18,  2.92it/s]Input ids are automatically padded from 27 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  35%|███▌      | 29/82 [00:10<00:18,  2.93it/s]Input ids are automatically padded from 47 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  49%|████▉     | 40/82 [00:13<00:14,  2.94it/s]Input ids are automatically padded from 31 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  56%|█████▌    | 46/82 [00:15<00:12,  2.95it/s]Input ids are automatically padded from 57 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  57%|█████▋    | 47/82 [00:16<00:11,  2.95it/s]Input ids are automatically padded from 49 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  68%|██████▊   | 56/82 [00:19<00:08,  2.95it/s]Input ids are automatically padded from 54 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  79%|███████▉  | 65/82 [00:22<00:05,  2.88it/s]Input ids are automatically padded from 52 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  88%|████████▊ | 72/82 [00:24<00:03,  2.91it/s]Input ids are automatically padded from 33 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  96%|█████████▋| 79/82 [00:27<00:01,  2.94it/s]Input ids are automatically padded from 30 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  98%|█████████▊| 80/82 [00:27<00:00,  2.94it/s]Input ids are automatically padded from 72 to 512 to be a multiple of `config.attention_window`: 512\n","Training: 100%|██████████| 82/82 [00:28<00:00,  2.90it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.37777777777777777, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6997042596340179}\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training:  18%|█▊        | 15/82 [00:05<00:22,  2.94it/s]Input ids are automatically padded from 41 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  40%|████      | 33/82 [00:11<00:16,  2.94it/s]Input ids are automatically padded from 43 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  44%|████▍     | 36/82 [00:12<00:15,  2.94it/s]Input ids are automatically padded from 18 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  96%|█████████▋| 79/82 [00:26<00:01,  2.93it/s]Input ids are automatically padded from 21 to 512 to be a multiple of `config.attention_window`: 512\n","Training: 100%|██████████| 82/82 [00:28<00:00,  2.93it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.49it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.37777777777777777, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6986220479011536}\n"]},{"output_type":"stream","name":"stderr","text":["Training:  62%|██████▏   | 51/82 [00:17<00:10,  2.95it/s]Input ids are automatically padded from 17 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  70%|██████▉   | 57/82 [00:19<00:08,  2.95it/s]Input ids are automatically padded from 19 to 512 to be a multiple of `config.attention_window`: 512\n","Training: 100%|██████████| 82/82 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.37777777777777777, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6963724792003632}\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.64it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["###########################################################################################################################\n","\r\r\r\r\r Fold 2/10 \n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training:  43%|████▎     | 35/82 [00:11<00:16,  2.94it/s]Input ids are automatically padded from 14 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  85%|████████▌ | 70/82 [00:23<00:04,  2.93it/s]Input ids are automatically padded from 28 to 512 to be a multiple of `config.attention_window`: 512\n","Training:  91%|█████████▏| 75/82 [00:25<00:02,  2.91it/s]Input ids are automatically padded from 37 to 512 to be a multiple of `config.attention_window`: 512\n","Training: 100%|██████████| 82/82 [00:28<00:00,  2.93it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.48it/s]"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4888888888888889, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6913121938705444}\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:27<00:00,  2.93it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4888888888888889, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6914311647415161}\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training:  10%|▉         | 8/82 [00:02<00:25,  2.91it/s]Input ids are automatically padded from 20 to 512 to be a multiple of `config.attention_window`: 512\n","Training: 100%|██████████| 82/82 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4888888888888889, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6908324360847473}\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.67it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["###########################################################################################################################\n","\r\r\r\r\r Fold 3/10 \n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 82/82 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4888888888888889, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.696052759885788}\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4888888888888889, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6953030228614807}\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4888888888888889, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6942329704761505}\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.67it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["###########################################################################################################################\n","\r\r\r\r\r Fold 4/10 \n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 82/82 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4444444444444444, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6973792314529419}\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.49it/s]"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4444444444444444, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6972717940807343}\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4444444444444444, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6957558393478394}\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.68it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["###########################################################################################################################\n","\r\r\r\r\r Fold 5/10 \n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 82/82 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.48it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4666666666666667, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6955130100250244}\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 82/82 [00:28<00:00,  2.91it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4666666666666667, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6952126026153564}\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 82/82 [00:28<00:00,  2.91it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.49it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4666666666666667, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6942499876022339}\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.65it/s]\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"]},{"output_type":"stream","name":"stdout","text":["###########################################################################################################################\n","\r\r\r\r\r Fold 6/10 \n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.93it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4444444444444444, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.696278989315033}\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.93it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]"]},{"output_type":"stream","name":"stdout","text":["F1: 0.0\n","{'Validation accuracy': 0.4444444444444444, 'Validation Precision': 0.0, 'Validation Recall': 0.0, 'Validation F1': 0.0, 'Validation loss': 0.6950215995311737}\n"]},{"output_type":"stream","name":"stderr","text":["\n","/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.93it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.07692307692307693\n","{'Validation accuracy': 0.4666666666666667, 'Validation Precision': 1.0, 'Validation Recall': 0.04, 'Validation F1': 0.07692307692307693, 'Validation loss': 0.6938944458961487}\n","Best F1: 0.07692307692307693\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["###########################################################################################################################\n","\r\r\r\r\r Fold 7/10 \n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.91it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.08333333333333333\n","{'Validation accuracy': 0.5111111111111111, 'Validation Precision': 1.0, 'Validation Recall': 0.043478260869565216, 'Validation F1': 0.08333333333333333, 'Validation loss': 0.689715564250946}\n","Best F1: 0.08333333333333333\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.2857142857142857\n","{'Validation accuracy': 0.5555555555555556, 'Validation Precision': 0.8, 'Validation Recall': 0.17391304347826086, 'Validation F1': 0.2857142857142857, 'Validation loss': 0.689402312040329}\n","Best F1: 0.2857142857142857\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training:  12%|█▏        | 10/83 [00:03<00:24,  2.92it/s]Input ids are automatically padded from 16 to 512 to be a multiple of `config.attention_window`: 512\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.5\n","{'Validation accuracy': 0.6444444444444445, 'Validation Precision': 0.8888888888888888, 'Validation Recall': 0.34782608695652173, 'Validation F1': 0.5, 'Validation loss': 0.6885296404361725}\n","Best F1: 0.5\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.74it/s]\n"]},{"output_type":"stream","name":"stdout","text":["###########################################################################################################################\n","\r\r\r\r\r Fold 8/10 \n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.48648648648648657\n","{'Validation accuracy': 0.5777777777777777, 'Validation Precision': 0.75, 'Validation Recall': 0.36, 'Validation F1': 0.48648648648648657, 'Validation loss': 0.6916063129901886}\n","Best F1: 0.48648648648648657\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.5365853658536586\n","{'Validation accuracy': 0.5777777777777777, 'Validation Precision': 0.6875, 'Validation Recall': 0.44, 'Validation F1': 0.5365853658536586, 'Validation loss': 0.690888911485672}\n","Best F1: 0.5365853658536586\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.93it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.5652173913043478\n","{'Validation accuracy': 0.5555555555555556, 'Validation Precision': 0.6190476190476191, 'Validation Recall': 0.52, 'Validation F1': 0.5652173913043478, 'Validation loss': 0.6888225674629211}\n","Best F1: 0.5652173913043478\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.67it/s]\n"]},{"output_type":"stream","name":"stdout","text":["###########################################################################################################################\n","\r\r\r\r\r Fold 9/10 \n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.5405405405405406\n","{'Validation accuracy': 0.6222222222222222, 'Validation Precision': 0.7692307692307693, 'Validation Recall': 0.4166666666666667, 'Validation F1': 0.5405405405405406, 'Validation loss': 0.6882005035877228}\n","Best F1: 0.5405405405405406\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training:  99%|█████████▉| 82/83 [00:28<00:00,  2.94it/s]Input ids are automatically padded from 15 to 512 to be a multiple of `config.attention_window`: 512\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.93it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.50it/s]\n","/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.5365853658536585\n","{'Validation accuracy': 0.5777777777777777, 'Validation Precision': 0.6470588235294118, 'Validation Recall': 0.4583333333333333, 'Validation F1': 0.5365853658536585, 'Validation loss': 0.6873168349266052}\n"]},{"output_type":"stream","name":"stderr","text":["Training: 100%|██████████| 83/83 [00:28<00:00,  2.93it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.5365853658536585\n","{'Validation accuracy': 0.5777777777777777, 'Validation Precision': 0.6470588235294118, 'Validation Recall': 0.4583333333333333, 'Validation F1': 0.5365853658536585, 'Validation loss': 0.68519127368927}\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.73it/s]\n"]},{"output_type":"stream","name":"stdout","text":["###########################################################################################################################\n","\r\r\r\r\r Fold 10/10 \n","###########################################################################################################################\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.94it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.611111111111111\n","{'Validation accuracy': 0.6888888888888889, 'Validation Precision': 0.7333333333333333, 'Validation Recall': 0.5238095238095238, 'Validation F1': 0.611111111111111, 'Validation loss': 0.6891049146652222}\n","Best F1: 0.611111111111111\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.92it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.52it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.6285714285714286\n","{'Validation accuracy': 0.7111111111111111, 'Validation Precision': 0.7857142857142857, 'Validation Recall': 0.5238095238095238, 'Validation F1': 0.6285714285714286, 'Validation loss': 0.6885456442832947}\n","Best F1: 0.6285714285714286\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:558: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  warnings.warn(_create_warning_msg(\n","Training: 100%|██████████| 83/83 [00:28<00:00,  2.87it/s]\n","Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.51it/s]\n"]},{"output_type":"stream","name":"stdout","text":["F1: 0.6666666666666666\n","{'Validation accuracy': 0.6888888888888889, 'Validation Precision': 0.6666666666666666, 'Validation Recall': 0.6666666666666666, 'Validation F1': 0.6666666666666666, 'Validation loss': 0.6872119307518005}\n","Best F1: 0.6666666666666666\n"]},{"output_type":"stream","name":"stderr","text":["Evaluation: 100%|██████████| 2/2 [00:00<00:00,  2.74it/s]\n"]}]},{"cell_type":"code","source":["# Calculate average evaluation metrics over all folds\n","avg_test_loss = sum(test_losses) / 10\n","avg_accuracy = sum(accuracies) / 10\n","avg_precision = sum(precisions) / 10\n","avg_recall = sum(recalls) / 10\n","avg_f1_score = sum(f1_scores) / 10\n","\n","# Print average evaluation metrics\n","print('Average test-loss = ', avg_test_loss)\n","print('Average test-acc = ', avg_accuracy)\n","print()\n","print('All precisions : ',precisions)\n","print('Average test-P = ', avg_precision)\n","print()\n","print('All recalls : ',recalls)\n","print('Average test-R = ', avg_recall)\n","print()\n","print('All F1 : ',f1_scores)\n","print('Average test-F1 = ', avg_f1_score)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kNgXVTUyW0Jk","executionInfo":{"status":"ok","timestamp":1711701466254,"user_tz":-60,"elapsed":2,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"7663b54d-b8ce-4159-e01c-4f87733dac30"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Average test-loss =  0.6900768399238586\n","Average test-acc =  0.5623112659698025\n","\n","All precisions :  [0.0, 0.0, 0.0, 0.0, 0.0, 0.5, 0.75, 0.8666666666666667, 0.6428571428571429, 0.6666666666666666]\n","Average test-P =  0.3426190476190476\n","\n","All recalls :  [0.0, 0.0, 0.0, 0.0, 0.0, 0.058823529411764705, 0.2857142857142857, 0.5652173913043478, 0.45, 0.631578947368421]\n","Average test-R =  0.1991334153798819\n","\n","All F1 :  [0.0, 0.0, 0.0, 0.0, 0.0, 0.10526315789473684, 0.4137931034482759, 0.6842105263157895, 0.5294117647058824, 0.6486486486486486]\n","Average test-F1 =  0.23813272010133332\n"]}]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["12zXxnbaj-iv","Ehb_wKBKrU1r","9bN2pP6POZ3N","nuPxsPMH8bYc","MhTiYrbQ5SYv","BLcFXvsoK9_k","_1t62yRTsJXf","Tx-K17cxsTnm","VO45b5vWDChB"],"gpuType":"V100","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNYcCz7bEnnFYcw5yc/zxNi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"93dc5767cd7243d8a3fb53509ab955b6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fb9ba356864944d3ad6b795964dd6aea","IPY_MODEL_629e61b7ba0f49aca7d70999aa240cba","IPY_MODEL_771de774558244919b5249d68fea4ade"],"layout":"IPY_MODEL_dc90e1022e5d4f2b88e55c1e5543e3d6"}},"fb9ba356864944d3ad6b795964dd6aea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_91bbea735287448a85b6bc6f1f3c1190","placeholder":"​","style":"IPY_MODEL_3f46e2e2b6844c2e94c992129116b1d9","value":"config.json: 100%"}},"629e61b7ba0f49aca7d70999aa240cba":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1f25c7bf4f043a183f4e7524ffdf6f1","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f9403afb44c241c4a5b2348ca119642f","value":385}},"771de774558244919b5249d68fea4ade":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_450c2965248245ea94d33ff964772229","placeholder":"​","style":"IPY_MODEL_0f40a0d889a8440b8121f35de1a1cccc","value":" 385/385 [00:00&lt;00:00, 28.0kB/s]"}},"dc90e1022e5d4f2b88e55c1e5543e3d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91bbea735287448a85b6bc6f1f3c1190":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f46e2e2b6844c2e94c992129116b1d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1f25c7bf4f043a183f4e7524ffdf6f1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9403afb44c241c4a5b2348ca119642f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"450c2965248245ea94d33ff964772229":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0f40a0d889a8440b8121f35de1a1cccc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"93010b298a5d43ae80bb8f3290ab1e74":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b9b10eb7390f4471bba2506c55d20308","IPY_MODEL_28c238dc5b15412f900b82164bd7837b","IPY_MODEL_c43c2d8cf1c74f4594896bc49352f512"],"layout":"IPY_MODEL_a33c56c1ed834bfc889e4c9d728b1e84"}},"b9b10eb7390f4471bba2506c55d20308":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95f994316d68460e87b3724318ecc81f","placeholder":"​","style":"IPY_MODEL_359bf94703b3406b942d9c48cc38de18","value":"vocab.txt: 100%"}},"28c238dc5b15412f900b82164bd7837b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f791691cff054d638a56acf2d63e20b5","max":227845,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cf9e66c5b0fa4816a3c32a8b7e3dcd81","value":227845}},"c43c2d8cf1c74f4594896bc49352f512":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b26a3199b754562a82cef6ca6c57d75","placeholder":"​","style":"IPY_MODEL_b66b30b94a974c07b5280b269a97c005","value":" 228k/228k [00:00&lt;00:00, 1.38MB/s]"}},"a33c56c1ed834bfc889e4c9d728b1e84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95f994316d68460e87b3724318ecc81f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"359bf94703b3406b942d9c48cc38de18":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f791691cff054d638a56acf2d63e20b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf9e66c5b0fa4816a3c32a8b7e3dcd81":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2b26a3199b754562a82cef6ca6c57d75":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b66b30b94a974c07b5280b269a97c005":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"459e1d6c79484555a99cd8bc749a22a8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e68c79b15ea94bfe9376a204c87b0660","IPY_MODEL_49488eb5165f4b9987a92a46b85678da","IPY_MODEL_755133535cbb419f9e176a22c94f5e02"],"layout":"IPY_MODEL_d18f416c9d18423bbb917cefe66d674a"}},"e68c79b15ea94bfe9376a204c87b0660":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1dc508cdb6b941249c2e701ee51b5bea","placeholder":"​","style":"IPY_MODEL_0c65fb773cde4f7c93a907d71350dbf6","value":"pytorch_model.bin: 100%"}},"49488eb5165f4b9987a92a46b85678da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_06e36766a68c4418aff3836fac1f1698","max":442221694,"min":0,"orientation":"horizontal","style":"IPY_MODEL_896bbae8b78b4ac38d36638b8a29fa27","value":442221694}},"755133535cbb419f9e176a22c94f5e02":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a1e29d387ad49af98889df8d35797c0","placeholder":"​","style":"IPY_MODEL_51cf5dd55c0f4f489c4b6502a545faac","value":" 442M/442M [00:01&lt;00:00, 276MB/s]"}},"d18f416c9d18423bbb917cefe66d674a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dc508cdb6b941249c2e701ee51b5bea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c65fb773cde4f7c93a907d71350dbf6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06e36766a68c4418aff3836fac1f1698":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"896bbae8b78b4ac38d36638b8a29fa27":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0a1e29d387ad49af98889df8d35797c0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"51cf5dd55c0f4f489c4b6502a545faac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6d0daa3359fa4776a72499bf6ac569b1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3b2ffdfc221445c0a816e25e64d8ac56","IPY_MODEL_75acab21c1aa407e873e91821ff1d76d","IPY_MODEL_bd3cc6020caa42efab79b1ec9fd8287d"],"layout":"IPY_MODEL_8c9b86848df6449f9e8885474e067445"}},"3b2ffdfc221445c0a816e25e64d8ac56":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1527e14b7964894a8317d766ef34a08","placeholder":"​","style":"IPY_MODEL_0b93ee1964ba44468e8a67a67fb46992","value":"config.json: 100%"}},"75acab21c1aa407e873e91821ff1d76d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3661533b16b747d9ad4e54b1a0d49411","max":694,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d8120080864c465a87440d7cb3f0807e","value":694}},"bd3cc6020caa42efab79b1ec9fd8287d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0950e06f1d9442a48be082346c8dfa6a","placeholder":"​","style":"IPY_MODEL_aef75eaa33db48ac891b61dd88c57470","value":" 694/694 [00:00&lt;00:00, 51.5kB/s]"}},"8c9b86848df6449f9e8885474e067445":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1527e14b7964894a8317d766ef34a08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b93ee1964ba44468e8a67a67fb46992":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3661533b16b747d9ad4e54b1a0d49411":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d8120080864c465a87440d7cb3f0807e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0950e06f1d9442a48be082346c8dfa6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aef75eaa33db48ac891b61dd88c57470":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e59c2f0d8c5410e98fc32e1ff8caca3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e1ce5d8567254d2296f92a0d6e151893","IPY_MODEL_3b318a830b0a4ba9b08020960116f35c","IPY_MODEL_94ee0b38df2f49dbb31631d9856f0477"],"layout":"IPY_MODEL_ec5515f54df84ceaa43a8de76b8cd052"}},"e1ce5d8567254d2296f92a0d6e151893":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_63e9f5a3f4e7448383642fb83e5720db","placeholder":"​","style":"IPY_MODEL_836ce6f4d11a464a93945a5f5273746e","value":"vocab.json: 100%"}},"3b318a830b0a4ba9b08020960116f35c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ea13c88216054470ab115037fb3e250d","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6f12e9d5955b4a1689ed1f27542194f2","value":898823}},"94ee0b38df2f49dbb31631d9856f0477":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3f336b1618948e5a8770951605ce267","placeholder":"​","style":"IPY_MODEL_94d84a1fb97d4fd1a12942ddc965aa71","value":" 899k/899k [00:00&lt;00:00, 2.73MB/s]"}},"ec5515f54df84ceaa43a8de76b8cd052":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63e9f5a3f4e7448383642fb83e5720db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"836ce6f4d11a464a93945a5f5273746e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ea13c88216054470ab115037fb3e250d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f12e9d5955b4a1689ed1f27542194f2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c3f336b1618948e5a8770951605ce267":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94d84a1fb97d4fd1a12942ddc965aa71":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3aa4654438048d797130812b6591873":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb21b328ace4417a9d888554fa73f1b3","IPY_MODEL_04364746588747268e677e46e73c96e6","IPY_MODEL_190425f648d541dcbd4f8f3a9edfb6a5"],"layout":"IPY_MODEL_68091ae0bb5c4dd49f64fbeba41ca81a"}},"eb21b328ace4417a9d888554fa73f1b3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a9645d296c954557b97630d39191fb46","placeholder":"​","style":"IPY_MODEL_3c038d66ddab43f5a5161fc63f2e0030","value":"merges.txt: 100%"}},"04364746588747268e677e46e73c96e6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb6bacbb19d24f00b025d13874d3c387","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_49af9a7429ba48a9aece4567decab847","value":456318}},"190425f648d541dcbd4f8f3a9edfb6a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa99ee51206f4d9abbdc6a4486e5d789","placeholder":"​","style":"IPY_MODEL_9b6e6fc16952420f9ff378822af302e9","value":" 456k/456k [00:00&lt;00:00, 1.87MB/s]"}},"68091ae0bb5c4dd49f64fbeba41ca81a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a9645d296c954557b97630d39191fb46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c038d66ddab43f5a5161fc63f2e0030":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb6bacbb19d24f00b025d13874d3c387":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49af9a7429ba48a9aece4567decab847":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fa99ee51206f4d9abbdc6a4486e5d789":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b6e6fc16952420f9ff378822af302e9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"06f4b39ca8fa4491bc71ec23783360b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0a19050189504bb398c11db760b80cd7","IPY_MODEL_b991414540e2463cbdb4cbbb54907b84","IPY_MODEL_a0b78d2b4f6a4cfdb56659fd247b3f23"],"layout":"IPY_MODEL_f8d561e942624d9ab529b9be9b07216b"}},"0a19050189504bb398c11db760b80cd7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_db8214f094f044ee9c8b7ce870a80f01","placeholder":"​","style":"IPY_MODEL_ecd76a9456f444f0877e37fac023a55d","value":"tokenizer.json: 100%"}},"b991414540e2463cbdb4cbbb54907b84":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9fcc7ba7f82a49e3b6f2497c2d70213c","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6f382a34d4d545d384717fbb6515f8d3","value":1355863}},"a0b78d2b4f6a4cfdb56659fd247b3f23":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d223a7594e84aa29961b19ad9b9060f","placeholder":"​","style":"IPY_MODEL_142761b6a3524762ae77e0179d2f21fa","value":" 1.36M/1.36M [00:00&lt;00:00, 3.68MB/s]"}},"f8d561e942624d9ab529b9be9b07216b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db8214f094f044ee9c8b7ce870a80f01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecd76a9456f444f0877e37fac023a55d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9fcc7ba7f82a49e3b6f2497c2d70213c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f382a34d4d545d384717fbb6515f8d3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d223a7594e84aa29961b19ad9b9060f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"142761b6a3524762ae77e0179d2f21fa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3c999d6415604500b8ad52f31704a934":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2804fa5a9e7459e90e81c5224d952b5","IPY_MODEL_f9cac0759dc148cdb4a45a5dce840cef","IPY_MODEL_288a0bb320744111b3c000ab84319fc5"],"layout":"IPY_MODEL_d8a1f6e43d364c50bc4abc778cdbd8fc"}},"d2804fa5a9e7459e90e81c5224d952b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0974544f750c4985bf75771201ff91b5","placeholder":"​","style":"IPY_MODEL_bb0013213e944639be0cc6e6db225b90","value":"pytorch_model.bin: 100%"}},"f9cac0759dc148cdb4a45a5dce840cef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b48487ea68046fd9fc4858f476fb495","max":597257159,"min":0,"orientation":"horizontal","style":"IPY_MODEL_213bdfd8649441488365a403ff0761cc","value":597257159}},"288a0bb320744111b3c000ab84319fc5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2036e4bd93e54f098c81992c519ad8b1","placeholder":"​","style":"IPY_MODEL_54e28b201d2842108f76beee62d1cabe","value":" 597M/597M [00:01&lt;00:00, 375MB/s]"}},"d8a1f6e43d364c50bc4abc778cdbd8fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0974544f750c4985bf75771201ff91b5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bb0013213e944639be0cc6e6db225b90":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b48487ea68046fd9fc4858f476fb495":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"213bdfd8649441488365a403ff0761cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2036e4bd93e54f098c81992c519ad8b1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54e28b201d2842108f76beee62d1cabe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"40051d90a7724e7b9895ca75731f7ac9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2791c002b68844e1be028f73ae67935b","IPY_MODEL_1c6e8c5fdd624c3989336f2a61754526","IPY_MODEL_caaaa55b33af478e89372b633a0a4bf1"],"layout":"IPY_MODEL_2a3f5a91e3d946ffa7fa730687df5b8c"}},"2791c002b68844e1be028f73ae67935b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44b2102d5bf346deb349d4dc4af176b9","placeholder":"​","style":"IPY_MODEL_a4b1019b91d14fa69065d561cb1f5da3","value":"config.json: 100%"}},"1c6e8c5fdd624c3989336f2a61754526":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1c93a7a55b2747eb8babbdef985c6793","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ef8a95c8928c44cc972cc88e3c3867aa","value":385}},"caaaa55b33af478e89372b633a0a4bf1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f5ea5887b4f6450c81129bc256fc4ac1","placeholder":"​","style":"IPY_MODEL_d854240f786e44eda00a795c4a1aa59a","value":" 385/385 [00:00&lt;00:00, 19.8kB/s]"}},"2a3f5a91e3d946ffa7fa730687df5b8c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44b2102d5bf346deb349d4dc4af176b9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a4b1019b91d14fa69065d561cb1f5da3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1c93a7a55b2747eb8babbdef985c6793":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef8a95c8928c44cc972cc88e3c3867aa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f5ea5887b4f6450c81129bc256fc4ac1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d854240f786e44eda00a795c4a1aa59a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c7b3a8fdc667424d972954763edd9f01":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b5b444322c0440a3bee1e1cae88d6b7f","IPY_MODEL_bfd25ea1a22c4fc4a4a8ad1cada12e01","IPY_MODEL_440f80f4a4d64821a8480d08f8375df8"],"layout":"IPY_MODEL_8a9b5d0b3ea041829e017d80f79c6173"}},"b5b444322c0440a3bee1e1cae88d6b7f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b94ffb7371d4479ab2de48d73d0c82b","placeholder":"​","style":"IPY_MODEL_1bf8d0a9bb414611a58f2212489d3559","value":"vocab.txt: 100%"}},"bfd25ea1a22c4fc4a4a8ad1cada12e01":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cf610fe5cd8b46939535ff3457d1f2c1","max":227845,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d7321657a9d647fb8563626666e044e5","value":227845}},"440f80f4a4d64821a8480d08f8375df8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_50a180f43c2f4721bac9266d8c52029d","placeholder":"​","style":"IPY_MODEL_78bda59e38ce406e8718fd0608f145b2","value":" 228k/228k [00:00&lt;00:00, 1.39MB/s]"}},"8a9b5d0b3ea041829e017d80f79c6173":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b94ffb7371d4479ab2de48d73d0c82b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1bf8d0a9bb414611a58f2212489d3559":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cf610fe5cd8b46939535ff3457d1f2c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d7321657a9d647fb8563626666e044e5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"50a180f43c2f4721bac9266d8c52029d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"78bda59e38ce406e8718fd0608f145b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}