{"cells":[{"cell_type":"markdown","source":["# Initialize"],"metadata":{"id":"WvoIaTkQLn6T"}},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"elapsed":1864,"status":"ok","timestamp":1713318319034,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"},"user_tz":-120},"id":"EVrqsFqwnXAs","outputId":"010c44c0-692d-47f3-f7b6-12037eaccb6b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content/drive/My Drive/Stage_Wassim_2024/Git_mock/Datasets'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":18}],"source":["#Technical stuff, mounting to drive, supressing some non useful warnings, and positioning into a certain directory\n","\n","from google.colab import drive\n","drive.mount(\"/content/drive\")\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning)\n","import os\n","%pwd"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"d70dBUQTu9cw","executionInfo":{"status":"ok","timestamp":1713318319034,"user_tz":-120,"elapsed":6,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"outputs":[],"source":["# Used libraries\n","# General\n","import numpy as np                              # math stuff\n","import pandas as pd                             # data manipulation\n","import matplotlib.pyplot as plt                 # plotting purposes(taking over the world)\n","import random                                   # random / seeds\n","from tqdm import tqdm                           # QOL looping\n","import json                                     # nested dictionaries, csv will mess up the data loader\n","import inspect\n","from typing import AnyStr, List                 #formatting stuff\n","\n","\n","# Classifying stuff\n","import tensorflow as tf\n","from sklearn.feature_extraction.text import TfidfVectorizer #TF_IDF\n","from sklearn.feature_extraction.text import CountVectorizer #Bag of words\n","from sklearn.linear_model import LogisticRegression #LR\n","from sklearn.metrics import precision_recall_fscore_support #All metrics\n","from sklearn.model_selection import KFold\n","#from bpemb import BPEmb #tokenize"]},{"cell_type":"code","source":["# VARIABLES\n","seed =  1000\n","lr = 0.000001351\n","weight_decay = 0.1\n","warmup_steps = 300\n","batch_size = 4\n","n_epochs = 3\n","\n","ff_dim = 256\n","n_heads = 1\n","n_layers = 1\n","dropout_prob = 0.1"],"metadata":{"id":"djDe_hss6Zpx","executionInfo":{"status":"ok","timestamp":1713318319034,"user_tz":-120,"elapsed":5,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["#setting the seed\n","\n","random.seed(seed)\n","np.random.seed(seed)\n","tf.random.set_seed(seed)"],"metadata":{"id":"am5UYBX-hd9O","executionInfo":{"status":"ok","timestamp":1713318319035,"user_tz":-120,"elapsed":5,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":["# Functions"],"metadata":{"id":"F2HcPG80LsUV"}},{"cell_type":"markdown","source":["## Data related functions\n"],"metadata":{"id":"xrw1UuqgZ7bU"}},{"cell_type":"code","source":["LABELS = {\n","    'non-check-worthy': 0,\n","    'check-worthy': 1\n","}"],"metadata":{"id":"btlIq1YvHnOj","executionInfo":{"status":"ok","timestamp":1713318321157,"user_tz":-120,"elapsed":7,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["def read_citation_detection_jsonl_single_line(jsonl_file: AnyStr):\n","\n","    with open(jsonl_file) as f:\n","        data = [json.loads(l.strip()) for l in f]\n","\n","    # Get sentences and labels\n","    dataset = [[s['text'], LABELS[s['label']]] for d in tqdm(data, desc=\"Processing data\") for s in d['samples']]\n","\n","    return pd.DataFrame(dataset, columns=['text', 'label'])"],"metadata":{"id":"soeC1yQwHlF2","executionInfo":{"status":"ok","timestamp":1713318321158,"user_tz":-120,"elapsed":7,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","execution_count":24,"metadata":{"id":"AnmF0rCb7dFw","executionInfo":{"status":"ok","timestamp":1713318321159,"user_tz":-120,"elapsed":8,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"outputs":[],"source":["def load_data() :\n","  # checking if I have the data locally\n","  if os.path.isfile('train.json') and os.path.isfile('test.json') and os.path.isfile('val.json') :\n","    # we load the data directly\n","    cw_train = pd.read_json('train.json')\n","    cw_test = pd.read_json('test.json')\n","    cw_val = pd.read_json('val.json')\n","  # if we don't have the data locally\n","  else :\n","    # we fetch the dataset\n","    CiteWorth = load_dataset('copenlu/citeworth')\n","    # we put the subset into data frames\n","    cw_train = pd.DataFrame(CiteWorth['train'])\n","    cw_test = pd.DataFrame(CiteWorth['test'])\n","    cw_val = pd.DataFrame(CiteWorth['validation'])\n","    # we save the subsets locally\n","    cw_train.to_json('train.json')\n","    cw_test.to_json('test.json')\n","    cw_val.to_json('val.json')\n","  return cw_train, cw_test, cw_val"]},{"cell_type":"code","execution_count":25,"metadata":{"id":"G-fsaG907pXG","executionInfo":{"status":"ok","timestamp":1713318321159,"user_tz":-120,"elapsed":7,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"outputs":[],"source":["def format_data(df) :\n","  samplesList = [item for row in tqdm(df['samples'], total=len(df), desc=\"Iterating over rows\") for item in row]\n","  formatted = pd.DataFrame(samplesList)\n","  formatted = formatted[['text','label']]\n","  formatted.label.replace({'non-check-worthy':0, 'check-worthy':1}, inplace=True)\n","  formatted.label = formatted.label.astype(np.int32)\n","  return formatted"]},{"cell_type":"code","source":["def check_data(df) :\n","  variable_name = [name for name, var in inspect.currentframe().f_back.f_locals.items() if var is df][0]\n","  print(\"Summary of the dataframe : (total=\"+str(df.shape[0])+\"):\")\n","  print(\"unique data : \")\n","  display(df[['text',\t'label']].nunique())\n","  print('--------------------------------------------------------------')\n","  print(f'Dataset shape : {df.shape[0]}')\n","  print('--------------------------------------------------------------')\n","  print('Number of non-check worthy : ',df[df['label'] == 0].shape[0])\n","  print('Number of check worthy : ',df[df['label'] == 1].shape[0])\n","  print('--------------------------------------------------------------')\n","  print('Number of NaN : ')\n","  print(df.isna().sum() )\n","  print('--------------------------------------------------------------')\n","  print(f'Number of duplicates : {df.duplicated().sum()}')\n","  print('In percentages',(df.duplicated().sum()/df.shape[0]*100),'%')\n","  print()\n","  display(df[df.duplicated(keep=False)].sort_values(by=list(df.columns)))\n","  print('--------------------------------------------------------------')\n","  print(f\"Number of same text but different labels : {df.duplicated(subset='text',keep=False).sum()} \")\n","  print('In percentages',(df.duplicated(subset='text',keep=False).sum()/df.shape[0]*100),'%')\n","  print()\n","  display(df[df.duplicated(subset='text',keep=False)].sort_values(by=list(df.columns)))\n","  print('--------------------------------------------------------------')\n"],"metadata":{"id":"pyzyttzamxxV","executionInfo":{"status":"ok","timestamp":1713318321160,"user_tz":-120,"elapsed":8,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["def clean_data(df, full= False) :\n","  # Deleting the null objects\n","  df = df.dropna()\n","  df.reset_index(drop=True, inplace=True)\n","  # deleting the duplicates\n","  df.drop_duplicates(inplace=True)\n","  df.reset_index(drop=True, inplace=True)\n","  #deleting same text different labels\n","  if full :\n","    df.drop_duplicates(subset=['text'],keep= False, inplace=True)\n","    df.reset_index(drop=True, inplace=True)\n","  print ('Done !')\n","  return(df)"],"metadata":{"id":"fXgNlotaD6DY","executionInfo":{"status":"ok","timestamp":1713318321160,"user_tz":-120,"elapsed":7,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def plotting_data(train,test,val) :\n","\n","  #General stuff\n","  colors = [\"firebrick\",\"yellowgreen\"]\n","  labels = {0: \"non check worthy (0)\", 1: \"check worthy (1)\"}\n","  datasets = [train, test, val]\n","  titles = ['Train', 'Test', 'Validation']\n","  #Plotting\n","  fig, axs = plt.subplots(nrows=1,ncols=3, figsize=(12,12))\n","\n","  for i, (dataset, title) in enumerate(zip(datasets, titles)):\n","      ax = axs[i]\n","      ax.pie(dataset['label'].value_counts().values,\n","               labels=[labels[label] for label in dataset['label'].value_counts().index],\n","               autopct=lambda x: str(round(x, 2)) + '%',\n","               colors=colors)\n","      for j, proportion in enumerate(dataset['label'].value_counts()):\n","        if j == 0 :\n","          ax.text(x=-0.50, y=0.25, s=str(round(proportion)))\n","        if j == 1 :\n","          ax.text(x=0.10, y=-0.75, s=str(round(proportion)))\n","      ax.set_title(title)\n","\n","  axs[1].annotate(\"CiteWorth label distribution\", xy=(0.5, -0.05), xycoords=\"axes fraction\", ha=\"center\", va=\"center\", fontsize=16)\n","  plt.show()\n"],"metadata":{"id":"pKMW5eX2PjPd","executionInfo":{"status":"ok","timestamp":1713318321160,"user_tz":-120,"elapsed":7,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["def to_Xy(df, reshape=False) :\n","  if reshape :\n","    X = df.values[:,0].reshape(df.shape[0],1)\n","    y = df.values[:,1].reshape(df.shape[0],1)\n","    y = y.astype('int32')\n","  else :\n","    X = df.values[:,0]\n","    y = df.values[:,1]\n","    y = y.astype('int32')\n","  return X,y"],"metadata":{"id":"P785dZlZL_is","executionInfo":{"status":"ok","timestamp":1713320022197,"user_tz":-120,"elapsed":217,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"execution_count":72,"outputs":[]},{"cell_type":"markdown","source":["## Experiment related functions\n"],"metadata":{"id":"R3sv1UueZ_Xp"}},{"cell_type":"code","source":["def exp_LR(train, test, C) :\n","\n","  X_train,y_train = to_Xy(train)\n","  X_test,y_test = to_Xy(test)\n","  # Vectorizing\n","  print(\"Vectorizing data...\")\n","  vectorizer = TfidfVectorizer()\n","\n","  vectorizer.fit(X_train)\n","\n","  Xtrain = vectorizer.transform(X_train)\n","  Xtest = vectorizer.transform(X_test)\n","\n","  print(\"Data vectorized.\")\n","  #Classifying\n","  print(\"Training classifier...\")\n","  classifier = LogisticRegression(penalty='l2', C=C, warm_start=True, class_weight='balanced')\n","  classifier.fit(Xtrain, y_train)\n","  print(\"Classifier trained.\")\n","  print(\"Predicting on test data...\")\n","  predicts = classifier.predict(Xtest)\n","  print(\"Prediction complete.\")\n","  print(\"Calculating metrics...\")\n","  P,R,F1,_ = precision_recall_fscore_support(y_test, predicts, average='binary')\n","  # wandb.run.summary[f'test-P'] = P\n","  # wandb.run.summary[f'test-R'] = R\n","  # wandb.run.summary[f'test-F1'] = F1\n","  print(f'test-P : {P}')\n","  print(f'test-R : {R}')\n","  print(f'test-F1 : {F1}')\n"],"metadata":{"id":"kt3WgGrec7T0","executionInfo":{"status":"ok","timestamp":1713318321161,"user_tz":-120,"elapsed":8,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"execution_count":30,"outputs":[]},{"cell_type":"code","source":["def exp_LR_2(ds1, ds2,  C, n_splits=10):\n","\n","    print(\"Initializing..\")\n","    precision_scores, recall_scores, f1_scores = [], [], []\n","    X_GIGA, y_GIGA = to_Xy(ds1)\n","    vectorizer = TfidfVectorizer()\n","    vectorizer.fit(X_GIGA)\n","    Xtrain = vectorizer.transform(X_GIGA)\n","\n","    # Classifying with k-fold cross-validation\n","    print(\"Training classifier with k-fold cross-validation...\")\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n","\n","    for fold_num, (train_index, test_index) in enumerate(kf.split(ds2), start = 1):\n","      print(\"###########################################################################################################################\")\n","      print(f\"\\r\\r\\r\\r\\r Fold {fold_num}/{10} \")\n","      print(\"###########################################################################################################################\")\n","    # Split the data into training and test sets for this fold\n","      print('splitting..')\n","      train_dset_fold = ds2.iloc[train_index]\n","      test_dset_fold = ds2.iloc[test_index]\n","    #create a vlaidation subsets :\n","\n","      X_test,y_test = to_Xy(test_dset_fold)\n","    # Initialize the vectorizer\n","      print('vectorizing..')\n","    #vectorizing\n","      Xtest = vectorizer.transform(X_test)\n","    #classifying\n","      print('classifying..')\n","      classifier = LogisticRegression(penalty='l2', C=C, warm_start=True, class_weight='balanced')\n","      classifier.fit(Xtrain, y_GIGA)\n","\n","      predicts = classifier.predict(Xtest)\n","      precision, recall, f1, _ = precision_recall_fscore_support(y_test, predicts, average='binary')\n","      precision_scores.append(precision)\n","      recall_scores.append(recall)\n","      f1_scores.append(f1)\n","      print(f\"{fold_num} Done !\")\n","\n","    mean_precision = np.mean(precision_scores)\n","    mean_recall = np.mean(recall_scores)\n","    mean_f1 = np.mean(f1_scores)\n","\n","    print(\"Mean Precision :\", mean_precision)\n","    print(\"Mean Recall : \", mean_recall)\n","    print(\"Mean F1 score : \", mean_f1)\n","    print(\"Precision scores for each fold : \", precision_scores)\n","    print(\"Recall scores for each fold : \", recall_scores)\n","    print(\"F1 scores for each fold : \", f1_scores)"],"metadata":{"id":"H6D6Cfo1tBTG","executionInfo":{"status":"ok","timestamp":1713321028594,"user_tz":-120,"elapsed":203,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"execution_count":88,"outputs":[]},{"cell_type":"code","source":["def exp_LR_3(ds1, C, n_splits=10):\n","\n","    precision_scores, recall_scores, f1_scores = [], [], []\n","\n","    # Classifying with k-fold cross-validation\n","    print(\"Training classifier with k-fold cross-validation...\")\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n","\n","    for fold_num, (train_index, test_index) in enumerate(kf.split(ds1), start = 1):\n","      print(\"###########################################################################################################################\")\n","      print(f\"\\r\\r\\r\\r\\r Fold {fold_num}/{10} \")\n","      print(\"###########################################################################################################################\")\n","    # Split the data into training and test sets for this fold\n","      print('splitting..')\n","      train_dset_fold = ds1.iloc[train_index]\n","      test_dset_fold = ds1.iloc[test_index]\n","    #create subsets :\n","      X_train,y_train = to_Xy(train_dset_fold)\n","      X_test,y_test = to_Xy(test_dset_fold)\n","    # Initialize the vectorizer\n","      print('vectorizing..')\n","      vectorizer = TfidfVectorizer()\n","    #vectorizing\n","      vectorizer.fit(X_train)\n","\n","      Xtrain = vectorizer.transform(X_train)\n","      Xtest = vectorizer.transform(X_test)\n","    #classifying\n","      print('classifying..')\n","      classifier = LogisticRegression(penalty='l2', C=C, warm_start=True, class_weight='balanced')\n","      classifier.fit(Xtrain, y_train)\n","\n","      predicts = classifier.predict(Xtest)\n","      precision, recall, f1, _ = precision_recall_fscore_support(y_test, predicts, average='binary')\n","      precision_scores.append(precision)\n","      recall_scores.append(recall)\n","      f1_scores.append(f1)\n","      print(f\"{fold_num} Done !\")\n","    mean_precision = np.mean(precision_scores)\n","    mean_recall = np.mean(recall_scores)\n","    mean_f1 = np.mean(f1_scores)\n","\n","    print(\"Mean Precision :\", mean_precision)\n","    print(\"Mean Recall : \", mean_recall)\n","    print(\"Mean F1 score : \", mean_f1)\n","    print(\"Precision scores for each fold : \", precision_scores)\n","    print(\"Recall scores for each fold : \", recall_scores)\n","    print(\"F1 scores for each fold : \", f1_scores)"],"metadata":{"id":"uJPZhsfMK8lQ","executionInfo":{"status":"ok","timestamp":1713320332563,"user_tz":-120,"elapsed":222,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}}},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":["# Data"],"metadata":{"id":"xwK8OSsmLwao"}},{"cell_type":"code","source":["## Use this Cell to position yourself at your data folder\n","#%cd Use This cell to access your data folder\n","%ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O_HH8-DaoFsS","executionInfo":{"status":"ok","timestamp":1713318334738,"user_tz":-120,"elapsed":5,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"885f7b51-3882-4bf5-cdef-073630763799"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: 'drive/MyDrive/Stage_Wassim_2024/Git_mock/Datasets'\n","/content/drive/MyDrive/Stage_Wassim_2024/Git_mock/Datasets\n","test.jsonl  train.jsonl  twt_FULL.jsonl  val.jsonl\n"]}]},{"cell_type":"code","execution_count":35,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19653,"status":"ok","timestamp":1713318358054,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"},"user_tz":-120},"id":"Aj4HkOiK-53a","outputId":"d98d9ece-8af0-4a97-d01c-96796dda31a1"},"outputs":[{"output_type":"stream","name":"stderr","text":["Processing data: 100%|██████████| 169015/169015 [00:01<00:00, 89762.22it/s] \n","Processing data: 100%|██████████| 20995/20995 [00:00<00:00, 45694.04it/s]\n","Processing data: 100%|██████████| 20990/20990 [00:00<00:00, 45239.14it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Done !\n","Done !\n","Done !\n","Done !\n","Done !\n"]},{"output_type":"stream","name":"stderr","text":["Processing data: 100%|██████████| 415/415 [00:00<00:00, 310883.40it/s]\n"]}],"source":["train  = read_citation_detection_jsonl_single_line('train.jsonl')\n","test = read_citation_detection_jsonl_single_line('test.jsonl')\n","val = read_citation_detection_jsonl_single_line('val.jsonl')\n","\n","train_clean = clean_data(train)\n","test_clean = clean_data(test)\n","val_clean = clean_data(val)\n","train_clean = clean_data(train, full=True)\n","val_clean = clean_data(val, full=True)\n","\n","twtfull = read_citation_detection_jsonl_single_line('twt_FULL.jsonl')"]},{"cell_type":"markdown","source":["# Experiments"],"metadata":{"id":"oIuz5gD39qwX"}},{"cell_type":"markdown","source":["## Experiment 1"],"metadata":{"id":"PIl26OGxLydH"}},{"cell_type":"code","source":["exp_LR(train_clean,test_clean, C = 0.1151)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"djqpkySjiMPq","executionInfo":{"status":"ok","timestamp":1713303304086,"user_tz":-120,"elapsed":80503,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"beead1fa-ea5a-43af-addb-4877d7f4566b"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Done !\n","Done !\n","Vectorizing data...\n","Data vectorized.\n","Training classifier...\n","Classifier trained.\n","Predicting on test data...\n","Prediction complete.\n","Calculating metrics...\n","test-P : 0.46659173707464535\n","test-R : 0.6485656565656566\n","test-F1 : 0.5427312378140179\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}]},{"cell_type":"markdown","source":["## Experiment 2\n"],"metadata":{"id":"AgA-YToaHN9f"}},{"cell_type":"code","source":["exp_LR_2(train_clean, twtfull, C = 0.1151)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5dA1tHjQJKbl","executionInfo":{"status":"ok","timestamp":1713321402053,"user_tz":-120,"elapsed":366184,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"64d7a413-630f-43be-adcf-ea932170d0ee"},"execution_count":89,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing..\n","Training classifier with k-fold cross-validation...\n","###########################################################################################################################\n"," Fold 1/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["1 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 2/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["2 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 3/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["3 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 4/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["4 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 5/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["5 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 6/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["6 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 7/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["7 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 8/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["8 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 9/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]},{"output_type":"stream","name":"stdout","text":["9 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 10/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n","10 Done !\n","Mean Precision : 0.4938489935491887\n","Mean Recall :  0.47429661535126605\n","Mean F1 score :  0.476742257570664\n","Precision scores for each fold :  [0.4166666666666667, 0.5833333333333334, 0.47368421052631576, 0.64, 0.34782608695652173, 0.35294117647058826, 0.46153846153846156, 0.6, 0.5625, 0.5]\n","Recall scores for each fold :  [0.5, 0.6363636363636364, 0.4090909090909091, 0.6153846153846154, 0.47058823529411764, 0.35294117647058826, 0.2857142857142857, 0.391304347826087, 0.45, 0.631578947368421]\n","F1 scores for each fold :  [0.45454545454545453, 0.6086956521739131, 0.43902439024390244, 0.6274509803921569, 0.39999999999999997, 0.35294117647058826, 0.35294117647058826, 0.47368421052631576, 0.5, 0.5581395348837209]\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}]},{"cell_type":"markdown","source":["## Experiment 3"],"metadata":{"id":"H8IHiGKoHVN9"}},{"cell_type":"code","source":["exp_LR_3(twtfull, C = 0.1151)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0EID0zAjLmVT","executionInfo":{"status":"ok","timestamp":1713321515447,"user_tz":-120,"elapsed":621,"user":{"displayName":"wassim ammar","userId":"14191714681496114525"}},"outputId":"26b2cef9-15a4-438f-c3ff-f18a2be78c0b"},"execution_count":93,"outputs":[{"output_type":"stream","name":"stdout","text":["Training classifier with k-fold cross-validation...\n","###########################################################################################################################\n","\r\r\r\r\r Fold 1/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n","1 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 2/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n","2 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 3/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n","3 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 4/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n","4 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 5/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n","5 Done !\n","###########################################################################################################################\n","\r\r\r\r\r Fold 6/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n","6 Done !\n","###########################################################################################################################\n"," Fold 7/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n","7 Done !\n","###########################################################################################################################\n"," Fold 8/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n","8 Done !\n","###########################################################################################################################\n"," Fold 9/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n","9 Done !\n","###########################################################################################################################\n"," Fold 10/10 \n","###########################################################################################################################\n","splitting..\n","vectorizing..\n","classifying..\n","10 Done !\n","Mean Precision : 0.56113051898521\n","Mean Recall :  0.578324222553594\n","Mean F1 score :  0.566768206903267\n","Precision scores for each fold :  [0.5909090909090909, 0.5238095238095238, 0.6842105263157895, 0.5652173913043478, 0.631578947368421, 0.5, 0.5217391304347826, 0.6521739130434783, 0.5416666666666666, 0.4]\n","Recall scores for each fold :  [0.65, 0.5, 0.5909090909090909, 0.5, 0.7058823529411765, 0.6470588235294118, 0.5714285714285714, 0.6521739130434783, 0.65, 0.3157894736842105]\n","F1 scores for each fold :  [0.6190476190476191, 0.5116279069767442, 0.6341463414634148, 0.5306122448979592, 0.6666666666666667, 0.5641025641025642, 0.5454545454545454, 0.6521739130434783, 0.5909090909090908, 0.35294117647058826]\n"]}]}],"metadata":{"accelerator":"TPU","colab":{"machine_shape":"hm","provenance":[],"collapsed_sections":["WvoIaTkQLn6T","F2HcPG80LsUV","oIuz5gD39qwX","PIl26OGxLydH","H8IHiGKoHVN9"],"authorship_tag":"ABX9TyP1WSytaMhaPvERbfA/kzjV"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}